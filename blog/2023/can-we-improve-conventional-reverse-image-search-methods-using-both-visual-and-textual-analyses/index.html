<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>By leveraging the capabilities of OpenAI’s CLIP to analyze both imagery and associated text, we can dramatically enhance search quality, providing a comprehensive, dual-faceted approach.</p> <figure><img alt="Illustration of a split digital panel. On the left, a magnifying glass scans a colorful artwork, converting it into binary codes. On the right, the panel displays a range of similar artworks based on the scan. Floating between the panels, the OpenAI logo shines brightly, emitting rays of light that merge the visual and textual data, emphasizing the dual approach of the technology." src="https://cdn-images-1.medium.com/max/1024/1*aXNzXp-XjKkJd_3hfWq1GA.png"><figcaption>Source: DALL-E-3</figcaption></figure> <p>In the digital age, even a single image can encapsulate vast amounts of information, narratives, and subtle details. But imagine if our machines could decipher images not merely by their visual elements, but also by the text they ‘read’. This principle is at the heart of OpenAI’s CLIP, a tool poised to bridge the gap between Computer Vision and Natural Language Processing. In this article, I’ll guide you step by step, demonstrating how we can harness the full potential of CLIP for advanced image searching.</p> <p>The GitHub link for this project can be found <a href="https://github.com/R-Suresh07/Reverse-Image-Search-CLIP.git" rel="external nofollow noopener" target="_blank">here</a>.</p> <p><strong>Problem Statement</strong></p> <p>Our objective is to illustrate that the integration of textual features, extracted by CLIP, along with image features derived from the same source, can enhance the overall accuracy. This combined approach may yield superior results compared to traditional reverse image search systems that rely solely on image similarity.</p> <p><strong>Value Proposition of the project</strong></p> <ul> <li> <strong>Superior Image Search</strong>: This project utilizes OpenAI’s CLIP to analyze both images and their associated text, significantly improving the quality of search results by providing a more comprehensive approach.</li> <li> <strong>Understanding Context and Text</strong>: Unlike traditional reverse image search systems that rely solely on image similarity, this project can find matches based on patterns, context, and the text within the image, addressing the limitations of traditional systems.</li> <li> <strong>Comprehensive Image Analysis</strong>: By integrating text similarity capabilities of CLIP with image similarity methods, the project can deliver more accurate and relevant search results. It can also handle cases where the exact image or brand is not in the dataset but a close match is available, offering a more holistic understanding of images.</li> </ul> <p><strong>What is Reverse Image Search?</strong></p> <p>Reverse image search is like asking the search engine, “Where else have you seen this picture?” Instead of typing in words to find images, you use an image to find similar or related images and information about them.</p> <p>Traditionally, reverse image search works by comparing the uploaded image’s patterns, colors, and certain defining features against a vast database of images. Popular platforms like Google Images and TinEye have made this functionality widely accessible.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/320/1*VARLpznQbggywfmKGP7x6w.png"><figcaption>Google reverse image search</figcaption></figure> <p>But these conventional methods have their downsides:</p> <ul> <li> <em>Limited Context Understanding:</em> These systems focus on pattern and color matching, often lacking context or deeper understanding of the image’s content.</li> <li> <em>Misidentification:</em> Similar patterns or colors in different images might lead to irrelevant search results.</li> <li> <em>Inability to Extract Textual Information: </em>Most traditional systems struggle with understanding or matching based on textual content within images.</li> </ul> <p>Now this is where CLIP steps in.</p> <p>CLIP, developed by OpenAI, can understand both visual and textual content in images. This means it can find matches based on patterns and context, as well as the text within the image. It addresses the downsides by offering a more holistic image understanding, thereby potentially providing more accurate and relevant search results.</p> <p>For more in-depth information on CLIP, you can check out my previous article where I delved deeper into CLIP.</p> <p><a href="https://medium.com/@sureshraghu0706/image-aesthetics-quantification-using-openai-clip-7bbb45e00147" rel="external nofollow noopener" target="_blank">Image aesthetics quantification using OpenAI CLIP</a></p> <p><strong>Methodology:</strong></p> <p>For this project, I’ve curated a dataset consisting of 30 liquor images, with their respective names used as the filenames. You’re welcome to modify this to fit your specific requirements. Here’s a glimpse of the dataset:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/422/1*c96m7l8UwHc1OZx_IFlKCA.png"><figcaption>Dataset Sample</figcaption></figure> <p><a href="http://dummy" rel="external nofollow noopener" target="_blank">Download the dataset here.</a></p> <p>There are two parts to the search: image similarity and text similarity. One might think that a simple image similarity with a good threshold would be suitable for this. However, let me provide you with a case where it doesn’t perform to the best of its capabilities.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/119/1*XZo9zSTEk5rAmnI7Bigk4w.png"></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/361/1*8w_90sVLSxCKHFzA33h22g.png"><figcaption>Absolut Citron( Left from Dataset, Right from Query)</figcaption></figure> <p>Let’s imagine a user is searching for the image shown on the right. Meanwhile, the image on the left represents our database’s entry. The two images differ, perhaps because the one on the right is from a recent bottle redesign or represents a special edition. Consequently, it doesn’t match the design stored in our database.</p> <p>When we apply only the Image Similarity technique to the image on the right, the results are as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/410/1*k2gDlkcXV16eIPrvCP-cHg.png"><figcaption>Results from regular image similarity</figcaption></figure> <p>Interestingly, the most relevant result displays a completely different brand. This discrepancy might arise because the shape and color of the bottle on the right bear a closer resemblance to Bacardi Ocho rather than Absolut Citron. Furthermore, observing the remaining results, one would logically expect all the Absolut bottles to rank higher before showcasing other brands. However, that isn’t the case here.</p> <p>To rectify this, we can enhance our search by extracting text features from the bottle and comparing them with the labels of all the bottles in our database. When employing the search logic I’ve developed (which I’ll explain shortly), the results are significantly improved:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/368/1*wTOaH1T855N8kQWe6ZBB2w.png"><figcaption>Results from combining image and text features</figcaption></figure> <p>As evident, the search outcomes are now much more relevant, with other pertinent SKUs making their appearance.</p> <p><strong>Implementation:</strong></p> <p>We will be using Python 3.10 to implement the search. We start by first installing all of the following dependencies. We will explore the usage of each one shortly.</p> <pre>!pip install numpy==1.24.4<br>!pip install pandas==2.0.3<br>!pip install Pillow==10.1.0<br>!pip install Requests==2.31.0<br>!pip install streamlit==1.27.2<br>!pip install torch<br>!pip install transformers</pre> <p>Let’s also get all the imports and loading the model out of the way:</p> <pre>import torch<br>from PIL import Image<br>from transformers import AutoProcessor, CLIPModel<br>import torch.nn as nn<br>import requests<br>from io import BytesIO<br>import os<br>import numpy as np<br>import matplotlib.pyplot as plt<br>import pickle<br>device = torch.device('cuda' if torch.cuda.is_available() else "cpu")<br>processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")<br>model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)</pre> <p>Here we use the transformers library to import the base model of CLIP. We are using the base model just for demonstration purposes, but you can use larger models for better accuracy but just beware that larger models take more system requirements and cause more latency for giving results. You could also use the <a href="https://huggingface.co/sentence-transformers/clip-ViT-B-32" rel="external nofollow noopener" target="_blank">sentence-transformers</a> library to use CLIP in your program.</p> <p>As mentioned earlier, the logic is split into two parts:</p> <ul> <li>Image Similarity</li> <li>Text Similarity</li> </ul> <p><strong>Image Similarity</strong></p> <p>First, let's write a couple of helper functions.</p> <pre>def load_image(image_path):<br>    if image_path.startswith("http://") or image_path.startswith("https://"):<br>        return Image.open(requests.get(image_path, stream=True).raw)<br>    else:<br>        return Image.open(image_path)<br>def cosine_similarity(vec1, vec2):<br># Compute the dot product of vec1 and vec2<br>    dot_product = np.dot(vec1, vec2)<br>    <br>    # Compute the L2 norm of vec1 and vec2<br>    norm_vec1 = np.linalg.norm(vec1)<br>    norm_vec2 = np.linalg.norm(vec2)<br>    <br>    # Compute the cosine similarity<br>    similarity = dot_product / (norm_vec1 * norm_vec2)<br>    <br>    return similarity</pre> <p>The load_image() function takes a URL, whether it be a local or an online URL, and converts it into a PIL Image object that we can manipulate in Python. The cosine_similarity() function is used to compute the similarity between two NumPy vectors. If you want to learn more about Cosine Similarity, follow the link below the image to an insightful article by AITechTrend, where they explain it in greater detail.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/643/1*0JblW1Xw81hU3w5CXdOM5w.png"><figcaption><a href="https://aitechtrend.com/how-cosine-similarity-can-improve-your-machine-learning-models/" rel="external nofollow noopener" target="_blank">Cosine Similarity</a></figcaption></figure> <pre>images = []<br>img_skus=[]<br>for root, dirs, files in os.walk(r"/Liquor_images"):<br>    for file in files:<br>        if file.endswith('png'):<br>            images.append(root  + '/'+ file)<br>img_skus=images.copy()<br>for i in range(len(img_skus)):<br>    img_skus[i]=os.path.basename(img_skus[i])[:-4]</pre> <p>We begin by loading all the image file paths from our dataset into the images array, and their corresponding names into the img_skus array. The dataset is stored in the same directory, under a folder named Liquor_images.</p> <pre>def extract_features_clip(image):<br>    with torch.no_grad():<br>        inputs1 = processor(images=image, return_tensors="pt").to(device)<br>        image_features = model.get_image_features(**inputs1)<br>    return image_features</pre> <p>Here, we’ve written a function called extract_features_clip(). This function takes an Image object as input and returns all the image features extracted from it using CLIP.</p> <pre>total_image_features=[]<br>for image_path in images:<br>    img = load_image(image_path)<br>    clip_feature = extract_features_clip(img)<br>    total_image_features.append(clip_feature)</pre> <p>Next, we iterate over the images array, converting the images at each filepath into image vectors. These vectors are stored in the total_image_features array. This is done by passing the images through the extract_features_clip() function that we wrote earlier.</p> <pre>filename='images.pkl'<br>with open(filename,'wb') as f:<br>    pickle.dump(total_image_features,f)</pre> <p>We then store all our embeddings in a pickle file. This allows us to save them on a storage device, from where we can retrieve them later. This eliminates the need to recompute the embeddings every time we want to perform a search. This completes all the preprocessing needed for the images.</p> <pre>with open(filename,'rb') as f:<br>    features=pickle.load(f)<br><br>def image_similarity(url):<br>    source=url<br>    image = load_image(source)<br>    #image=url<br>    image_features=extract_features_clip(image)<br>    similarity_scores = [cosine_similarity(image_features, i[0]) for i in features]<br>    merged_dict=dict(zip(img_skus,similarity_scores))<br>    sorted_dict = dict(sorted(merged_dict.items(), key=lambda item: item[1],reverse=True))<br>    return sorted_dict</pre> <p>Finally, we arrive at the part where we compute the image similarity. We first retrieve the image embeddings we previously created from the pickle file and store them in an array. When the image_similarity() function receives a URL, it converts the URL into an Image object using load_image(). We then compute the image embeddings by passing it through extract_features_clip(). Next, we compute the cosine similarity between the query image and all the vectors in the database using the cosine_similarity() function. We create a dictionary where the keys are the names of the most similar bottles and the values are their cosine similarity scores with the input image. Finally, we return this dictionary to the user which contains the search results.</p> <p>Now let’s move onto the text similarity score.</p> <p><strong>Text Similarity</strong></p> <pre>product_names = [<br>    "Royal Salute 21 YO 70cl",<br>    "Aberfeldy 12 YO Sp",<br>    "Aberlour A Bunadh 70cl",<br>    "Absolut 100 100cl",<br>    "Absolut Citron 100cl",<br>    "Absolut Elyx 100cl",<br>    "Absolut Lime 100cl",<br>    "Absolut Mandrin 100cl",<br>    "Absolut Mango 100cl",<br>    "Absolut Pears 100cl",<br>    "Absolut Vodka Blue 100cl",<br>    "Absolut Vodka Grapefruit 100cl",<br>    "Bacardi Carta Blanca 100cl",<br>    "Bacardi Ocho",<br>    "Baileys Original 100 CL",<br>    "Baileys Salted Caramel 100cl",<br>    "Ballantine's 30 YO 70cl",<br>    "Beluga Celebration 100 CL",<br>    "Beluga Gold Line 1 L",<br>    "Ballantine's Finest Twin Pack Cary 2x100cl"<br>]<br><br>class_names=product_names<br># Compute text features for class names<br>text_inputs = processor(text=class_names, return_tensors="pt", padding=True).to(device)<br>with torch.no_grad():<br>    text_features = model.get_text_features(**text_inputs)</pre> <p>We create an array called product_names that contains the names of all the images in our dataset. Ideally, these names should closely match what is written on the labels of the liquor bottles. This array will serve as the class names to which CLIP will classify images. We then pass product_names to CLIP to extract all text features and store them in an array called text_features. You can also store this array as a pickle file for later retrieval, similar to how we stored image embeddings in a pickle file.</p> <pre>def predict(img_url):<br>    image1 = load_image(img_url)<br>    #image1=img_url<br>    image_features1=extract_features_clip(image1)<br>    cos_sim = nn.CosineSimilarity(dim=-1)<br>    similarity_scores = cos_sim(image_features1, text_features)<br>    similarity_scores=similarity_scores.tolist()<br>    merged_dict=dict(zip(product_names,similarity_scores))<br>    sorted_dict = dict(sorted(merged_dict.items(), key=lambda item: item[1],reverse=True))<br>    return sorted_dict</pre> <p>We then write a function called predict(), which takes in an image URL and converts it into an Image object using load_image(). We extract the image features from the query image using extract_features_clip(). We then compute the cosine similarity of the image vector to all textual vectors. Normally, this isn’t possible as image vectors and text vectors occupy different embedding spaces, but CLIP’s vectors are different. The entire design philosophy of CLIP is that both images and text share the same embedding space. This allows us to compare text vectors with image vectors. Returning to the program, we compute a dictionary similar to how we did in the image_similarity() function and return it to the user.</p> <p>Now that we have one function to extract textual features and another to extract image features, all we need to do is combine them and apply some sort of logic to rank them.</p> <pre>def image_search(url):<br>    text_similarity=predict(url)<br>    img_similarity=image_similarity(url)<br>    result={}<br>    for k,v in text_similarity.items():<br>        result[k]=0.5*text_similarity[k]*3+0.5*img_similarity[k]<br>    top_5_keys = sorted(result, key=result.get, reverse=True)[:5]<br>    return top_5_keys</pre> <p>We finally define a function called image_search(), which takes an image URL as input. This URL is passed to both the predict() and image_similarity() functions, and the score dictionaries are stored as text_similarity and img_similarity, respectively. We then create a final dictionary called result, where we compute the results and store the final ranking. Finally, we retrieve the top five keys with the highest values and return them to the user. If you observe the ranking logic, you’ll notice that I’ve assigned a 75% weightage to the text similarity score and a 25% weightage to the image similarity score. This is because when comparing text to an image, you’ll find that the score is significantly lower than when comparing an image to another image. To compensate for this, I’ve arbitrarily multiplied the text similarity score by 3. You can fine-tune and adjust this value if you believe there’s a better one</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/418/1*jbpE8f7O7SptIyu08D_80w.png"></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/445/1*z58SR5MUNrM3DP_IxnQ1tA.png"><figcaption>Text Scores(Left) and Image Scores(Right)</figcaption></figure> <p><strong>Optional: Streamlit Demo</strong></p> <p>If you wish to demonstrate your search to someone, you can use the following code as a template. Just ensure that you adjust the predict() and image_similarity() functions to accept an Image object as input, rather than a URL.</p> <pre>import streamlit as st<br>st.header('Image Search App')<br><br>uploaded_file = st.file_uploader("Choose an image...", type=['png','jpg','jpeg'])<br>picture_width = st.sidebar.slider('Picture Width', min_value=100, max_value=500)<br>if uploaded_file is not None:<br>    image = Image.open(uploaded_file)<br>    st.subheader('Input', divider='rainbow')<br>    st.image(image, caption='Uploaded Image', width=picture_width)<br><br>        # Call your function with the uploaded image<br>    results = image_search(image)<br>    <br>    st.subheader('Results', divider='rainbow')<br>        # Display the results<br>    for product in results:<br>        product_image_path = os.path.join(r'Liquor_images', f'{product}.png')<br>        product_image = Image.open(product_image_path)<br>        st.image(product_image, caption=product, width=picture_width)</pre> <p><strong>Demonstration</strong></p> <p>Let’s put this system to the test. I will demonstrate the search on 3 images.</p> <p>Image 1</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ShXl-LJpC5NDELv0xFMCMg.png"><figcaption>Image of Bacardi Carta Blanca</figcaption></figure> <p>Results for Image 1</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/938/1*Wq0q9KvYR4kGRhw20XHwmw.png"><figcaption>Results page 1</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/936/1*naTPjIEWnDr2HtLUzP5I3Q.png"><figcaption>Results page 2</figcaption></figure> <p>Image 2</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/172/1*hPloDAS_12vPkENSm0gkhA.jpeg"><figcaption>Absolut Raspberry (Not in Dataset)</figcaption></figure> <p>Results for Image 2</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/924/1*vJAgYclbdIMgbHgH6cs4_Q.png"><figcaption>Results page 1</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/877/1*AQjCWfefTN8y9qfUztzyOQ.png"><figcaption>Results page 2</figcaption></figure> <p>We can see that even when the image isn’t in the dataset, but the brand is, the program is able to identify the most relevant brand. Moreover, the top result is the closest color to raspberry, which is grapefruit.</p> <p>Image 3</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*R4lS1r8U1kJbiyO-wv8uvw.jpeg"><figcaption>Jack Daniels (Neither the brand nor the image is in the dataset)</figcaption></figure> <p>Results for Image 3</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/939/1*AQgEbu7ArEupk84whvn6Tw.png"><figcaption>Results page 1</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/942/1*HH7_IUolZTlgOKBlHebTDw.png"><figcaption>Results page 2</figcaption></figure> <p>We can see that when the image is searched, if the exact SKU and brand aren’t in the dataset, it returns the image that looks closest to the provided image. Hence, the top three results are all brown-colored bottles, which resemble the provided Jack Daniel’s bottle the most. We can observe that the scores of these images are lower compared to Image 1 and Image 2, where we found exact matches either in the SKU or the brand.</p> <p><strong>Conclusion:</strong></p> <p>So that's about it folks, we have successfully enhanced the basic image similarity-based reverse image search methods by integrating the text similarity capabilities of CLIP which gives more intelligent results. If you found a flaw in my logic, feel free to message me.</p> <h3><em>Follow For More!</em></h3> <p><em>I try to implement a lot of theoretical concepts in the ML space, with an emphasis on practical and intuitive applications.</em></p> <p><em>Thanks for reading this article! If you have any questions, I will be happy to answer them. Feel free to message me on my </em><a href="http://linkedin.com/in/suresh-raghu" rel="external nofollow noopener" target="_blank"><em>LinkedIn</em></a><em> or my </em><a href="http://sureshraghu0706@gmail.com" rel="external nofollow noopener" target="_blank"><em>email</em></a><em> for other queries.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=31ed580be9d4" width="1" height="1" alt=""></p> </body></html>