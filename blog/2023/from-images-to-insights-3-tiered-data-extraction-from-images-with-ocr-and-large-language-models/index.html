<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Unpacking the Technology: How OCR and LLMs Are Redefining Data Capture from Images</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QVATTx6hbjur5bjMEmpI1Q.png"><figcaption>Source: DALL-E-3</figcaption></figure> <p>Recently, I embarked on a personal project to track my spending by extracting information from images of grocery store receipts, inspired by apps like <a href="https://happay.com/expense-management-software/" rel="external nofollow noopener" target="_blank">Happay</a> or <a href="https://www.zoho.com/in/expense/receipt-tracking/" rel="external nofollow noopener" target="_blank">Zoho</a>. As I delved deeper into the project, I quickly realized the complexity of the problem I was facing. The core issue was the inconsistency in format and information placement across different receipts. This variability meant that a singular model couldn’t reliably extract all the necessary data.</p> <p>This personal challenge echoed a larger, industry-wide dilemma: the difficulty of processing information from documents with diverse layouts, not just limited to receipts but extending to various other forms like boarding passes or invoices. Traditional extraction methods, such as rule-based systems, proved insufficient due to their rigidity and inability to adapt to the wide range of document designs.</p> <p>Driven by my own experiences and the broader implications of this issue, my project aimed to develop a solution that was both flexible and efficient for extracting information from a variety of document formats. I focused on leveraging <a href="https://en.wikipedia.org/wiki/Optical_character_recognition" rel="external nofollow noopener" target="_blank">OCR (Optical Character Recognition)</a> and <a href="https://huggingface.co/docs/transformers/llm_tutorial" rel="external nofollow noopener" target="_blank">Large Language Models (LLMs)</a> like ChatGPT. This approach marked a shift from using OCR solely for digitizing text to a comprehensive process where OCR-generated unstructured text is transformed into structured data using the advanced contextual capabilities of LLMs.</p> <p>The innovation of this method lies in its adaptability, capable of handling numerous document types without extensive training or specific rules for each format. My goal was to demonstrate an effective strategy for data extraction that could be a reference point for similar challenges faced in various business contexts. This project was not just a technical pursuit but a journey that stemmed from a personal need to find a practical solution to an everyday problem.</p> <p>The GitHub link of the project can be found <a href="https://github.com/R-Suresh07/Information-Extraction-from-Images.git" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>The Google Collab link of Methodology 3 can be found here:</p> <p><a href="https://colab.research.google.com/drive/14_WT-yUYJKfYrgLHKIE9nLc_EQIKOXRv?usp=sharing" rel="external nofollow noopener" target="_blank">Google Colaboratory</a></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OCYubeFJ9jdLmmgt.PNG"><figcaption>Source: <a href="https://www.semanticscholar.org/paper/Representation-Learning-for-Information-Extraction-Majumder-Potti/58877aa9aa2d09585a4ff4881b02cb1c7ff9bc28" rel="external nofollow noopener" target="_blank">[PDF] Representation Learning for Information Extraction from Form-like Documents | Semantic Scholar</a></figcaption></figure> <p><strong>Problem Statement</strong></p> <p>In the rapidly digitizing world, businesses and individuals frequently encounter the challenge of extracting precise information from a myriad of image-based documents like receipts, boarding passes, and invoices. Each of these document types comes with its unique layout and format, making the task of data extraction and processing both complex and time-consuming. Traditional methods often fall short due to their inflexibility and inability to handle the vast diversity in document designs.</p> <p>This project tackles the pressing need for a more adaptive and scalable approach to decipher and transform unstructured data from various image formats into structured, actionable information. It explores the intersection of Optical Character Recognition (OCR) and Large Language Models (LLMs) as a solution. The focus is on developing methods that not only accurately extract text from images but also intelligently interpret and organize this data, catering to the nuances of different document types.</p> <p>The aim is to bridge the gap between the simplicity of image-based data and the complexity of its extraction and structuring, providing efficient, cost-effective, and versatile solutions suitable for diverse applications in the digital era.</p> <p><strong>Value Propositions of this project</strong></p> <ol> <li> <strong>Versatility in Data Processing:</strong> Capable of handling a wide range of document formats and styles, making it suitable for various types of image-based data extraction tasks.</li> <li> <strong>Increased Efficiency:</strong> Streamlines the process of converting unstructured data from images into structured, usable formats, saving time and resources.</li> <li> <strong>Enhanced Accuracy:</strong> Combines the precision of advanced OCR technologies with the contextual understanding of LLMs, leading to more accurate data extraction and interpretation.</li> <li> <strong>Cost-Effectiveness:</strong> Offers solutions that range from open-source, free tools to more sophisticated, paid services, allowing for cost-effective implementation according to the user’s budget and needs.</li> <li> <strong>Data Security and Autonomy:</strong> Provides options that include in-house processing, minimizing data privacy concerns and ensuring greater control over the data extraction process.</li> <li> <strong>Scalability:</strong> Suitable for both small-scale individual use and large-scale enterprise applications, with potential for customization and scaling as per user requirements.</li> <li> <strong>Ease of Use:</strong> Despite the underlying complexity, the methodologies are designed to be accessible and implementable, even for those with limited technical expertise.</li> <li> <strong>Customizability:</strong> Offers the flexibility to mix and match OCR and LLM tools, and even fine-tune models for specific use cases, enhancing the relevance and effectiveness of the solutions.</li> <li> <strong>Innovation and Future-Proofing:</strong> Demonstrates the integration of cutting-edge AI technologies, positioning users at the forefront of technological advancements in data processing.</li> </ol> <p><strong>Exploration</strong></p> <p>Having encountered these challenges in my personal project, I realized that the solution lies not just in a single technology, but in a combination of tools and approaches. This realization led me to explore and experiment with three distinct methodologies, each offering its unique advantages and drawbacks in the context of information extraction from images.</p> <p>After discussing the advantages and disadvantages of each methodology, it’s important to not only understand them theoretically but also practically. To bridge this gap, I will be providing detailed, step-by-step <a href="https://github.com/R-Suresh07/Information-Extraction-from-Images.git" rel="external nofollow noopener" target="_blank">code implementations</a> in python for each of these three methodologies in the subsequent sections of this article.</p> <p><strong>Methodology 1: GPT-4 Vision (GPT-V) by OpenAI</strong></p> <p>The first methodology I explored leverages the innovative capabilities of GPT-4 Vision (GPT-V), a state-of-the-art <a href="https://huggingface.co/tasks/visual-question-answering" rel="external nofollow noopener" target="_blank">Visual Question Answering (VQA)</a> model developed by OpenAI. GPT-V stands out for its ability to analyze images and provide detailed information based on user queries. This model allows users to upload an image and ask specific questions about its content, to which GPT-V responds with precise and relevant information.</p> <p>In the context of extracting data from images, GPT-V offers a highly intuitive and efficient solution. The process is remarkably simple: you upload an image to the model, and GPT-V processes this image to return the required information in a structured JSON format. This data can then be easily stored and retrieved for later use.</p> <p><strong>Advantages of Using GPT-V:</strong></p> <ul> <li> <strong>Simplicity and Efficiency:</strong> The process of uploading an image and receiving data in JSON format is straightforward, minimizing the complexity typically associated with data extraction.</li> <li> <strong>High Effectiveness:</strong> GPT-V employs the sophisticated capabilities of the GPT-4 model, ensuring accurate and reliable data extraction from images.</li> </ul> <p><strong>Disadvantages of Using GPT-V:</strong></p> <ul> <li> <strong>Cost Considerations:</strong> Utilizing GPT-V can be costly, particularly when processing images with high resolution, which may be a significant factor for projects with budget constraints.</li> <li> <strong>Data Autonomy Concerns:</strong> As GPT-V is a third-party service, there are inherent issues regarding data privacy and control. This reliance on an external vendor may pose challenges for projects where data security and autonomy are paramount.</li> <li> <strong>API Latency and Rate Limits:</strong> There can be significant time delays in receiving information back from the API, and may have rate limits imposed on the account, affecting the scalability and responsiveness of the system.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*sPIBRjiw3FUSz16X.png"><figcaption>Source: <a href="https://community.openai.com/u/wclayf" rel="external nofollow noopener" target="_blank">Summary — wclayf — OpenAI Developer Forum</a></figcaption></figure> <p><strong>Methodology 2: Azure Cognitive Services and GPT 3.5 Turbo by OpenAI</strong></p> <p>The second methodology I explored combines the use of Optical Character Recognition (OCR) and a Large-Language Model (LLM) to effectively process and structure data extracted from images. This method starts with an OCR model to capture all textual information from the image, followed by the utilization of an LLM to organize this unstructured data into a desired format.</p> <p>For this approach, I employed <a href="https://azure.microsoft.com/en-us/products/ai-services/ai-vision" rel="external nofollow noopener" target="_blank">Azure Cognitive Services (ACS)</a> as the OCR tool and <a href="https://platform.openai.com/docs/api-reference" rel="external nofollow noopener" target="_blank">GPT 3.5 Turbo</a> for data structuring. ACS excels in extracting all text present in an image, converting it from a visual format into a machine-readable, unstructured textual format. Following this, GPT 3.5 Turbo’s JSON formatter function is used to restructure this raw text. The integration of ACS’s powerful OCR capabilities with the advanced data structuring ability of GPT 3.5 Turbo results in highly accurate information extraction. The outcome is not just a simple transcription of the image’s text but a well-organized, structured JSON representation of the information originally contained in the image.</p> <p><strong>Advantages of Using ACS and GPT 3.5:</strong></p> <ul> <li> <strong>Accurate OCR:</strong> ACS provides high-quality text extraction.</li> <li> <strong>Advanced Data Structuring: </strong>GPT 3.5 Turbo effectively transforms unstructured text into structured JSON.</li> <li> <strong>Quality Results:</strong> Combines two powerful tools for better accuracy.</li> </ul> <p><strong>Disadvantages of Using ACS and GPT 3.5:</strong></p> <ul> <li> <strong>Cost and Dependency:</strong> Although not as expensive as methodology 1, this approach still involves expenses for external services and dependency on third-party vendors.</li> <li> <strong>Data Privacy:</strong> Potential issues with data autonomy due to reliance on external processing.</li> <li> <strong>API Latency and Rate Limits:</strong> Similar to GPT-V, this method faces issues with time delays and potential rate limits on API usage, impacting real-time processing capabilities.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/1*b_29FYcpCJxEf4z8yhNKeg.png"><figcaption>Flowchart for Methodology 2 (Source: Me)</figcaption></figure> <p><strong>Methodology 3: PaddleOCR and Zephyr-7b</strong></p> <p>The final approach I explored shares a similar template with the second methodology but shifts its focus towards data security and latency, while still maintaining a strong emphasis on result accuracy.</p> <p>For the OCR component, I opted for <a href="https://pypi.org/project/paddleocr/" rel="external nofollow noopener" target="_blank">PaddleOCR</a>. This toolkit, built on the PaddlePaddle framework, supports over 80 languages and offers a comprehensive suite of tools for text recognition and detection. Additionally, PaddleOCR is equipped with features for data annotation and synthesis, and it is optimized for various deployment environments, including servers, mobile, embedded, and IoT devices. Its lightweight nature makes it an ideal choice for a production environment, particularly where resource efficiency is a priority.</p> <p>Moving to the Large Language Model (LLM) aspect, I selected <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha" rel="external nofollow noopener" target="_blank">Zephyr-7B</a> developed by HuggingFace. Zephyr-7B is a fine-tuned version of the Mistral-7B model, which has demonstrated impressive performance, rivaling GPT-3.5 Turbo in many benchmarks, including MT-Bench and AlpacaEval. The efficiency of Zephyr-7B, despite being a 7-billion parameter model, means it is accessible for consumer-grade hardware, making it a practical choice for various applications.</p> <p>For those interested in learning more about Zephyr-7B and its development, I recommend the article <a href="https://gathnex.medium.com/zephyr-7b-beats-chatgpt-huggingface-open-challenge-to-openai-bba2fceecd9c" rel="external nofollow noopener" target="_blank">“Zephyr 7B Beta beats ChatGPT: HuggingFace’s Open challenge to OpenAI” by Gathnex</a> (published in October 2023) and exploring its <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha" rel="external nofollow noopener" target="_blank">dedicated page on HuggingFace’s website</a>.</p> <p><strong>Advantages of using PaddleOCR and Zephyr-7B:</strong></p> <ul> <li> <strong>Cost-Efficient:</strong> No additional costs for external services.</li> <li> <strong>Data Security:</strong> Enhanced data privacy as processing is done within your ecosystem.</li> <li> <strong>Independence from APIs:</strong> The lack of reliance on external APIs eliminates concerns about rate limits and latency, with speed dependent only on the user’s hardware.</li> </ul> <p><strong>Disadvantages</strong> <strong>of using PaddleOCR and Zephyr-7B</strong>:</p> <ul> <li> <strong>Infrastructure Requirements:</strong> Requires significant computational resources for scaling.</li> <li> <strong>OCR Accuracy Variances:</strong> Paddle-OCR’s accuracy might not suffice for certain use cases.</li> <li> <strong>Inference Speed:</strong> Processing speed is contingent on the available hardware, which could be a limitation for users with less powerful systems.</li> </ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/404/1*BE2xtVSX4r0Xfo3KqqXD9g.png"></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/224/1*UpUX_JxXrXxSj2c8ENDeBQ.png"></figure> <p><strong>Transitioning to Implementations</strong></p> <p>Having explored the theoretical aspects and weighed the pros and cons of each methodology, it’s time to delve into the practical side. In the following sections, I will guide you through the step-by-step implementation of each methodology. These walkthroughs are designed to be clear and comprehensive, whether you’re a novice just starting out or an experienced developer looking to expand your toolkit.</p> <p><strong>Implementation of GPT-4 Vision (GPT-V)</strong></p> <p>Let’s first install the additional dependency required.</p> <pre>!pip install openai</pre> <p>Then let’s get all of our imports and API key sorted.</p> <pre>import base64<br>import requests<br>from openai import OpenAI<br># OpenAI API Key<br>api_key = "Insert-Your-OpenAi-API-Key-Here"<br>client = OpenAI(api_key = api_key)</pre> <p>To start using GPT-V, you first need to create an account on OpenAI’s website and then generate <a href="https://platform.openai.com/api-keys" rel="external nofollow noopener" target="_blank">API keys</a>. It’s important to note that GPT-V access requires at least one completed payment on your account. Once these prerequisites are met, OpenAI will provide you with access to GPT-V.</p> <p>Now let’s write a helper function.</p> <pre>def encode_image(image_path):<br>    with open(image_path, "rb") as image_file:<br>        return base64.b64encode(image_file.read()).decode('utf-8')<br><br>headers = {<br>        "Content-Type": "application/json",<br>        "Authorization": f"Bearer {api_key}"<br>}</pre> <p>Before making an API request, the local image must be converted into a format compatible with GPT-V. This is where the encode_image() function comes into play. It takes a locally stored image and converts it into its base64 encoded version, ready for the API request.</p> <p>Then we write the code which will help us send the API request.</p> <pre>def question_image(url,query):<br>    if url.startswith("http://")or url.startswith("https://"):<br>        response = client.chat.completions.create(<br>            model="gpt-4-vision-preview",<br>            messages=[<br>            {<br>            "role": "user",<br>            "content": [<br>                {"type": "text", "text": f"{query}"},<br>                    {<br>                    "type": "image_url",<br>                    "image_url": url,<br>                    },<br>                ],<br>            }<br>        ],<br>        max_tokens=1000,<br>            <br>        )<br>        return response.choices[0].message.content<br>    else:<br>        <br>        base64_image = encode_image(url)<br><br>        payload = {<br>            "model": "gpt-4-vision-preview",<br>            "messages": [<br>              {<br>                "role": "user",<br>                "content": [<br>                  {<br>                    "type": "text",<br>                    "text": f"{query}?"<br>                  },<br>                  {<br>                    "type": "image_url",<br>                    "image_url": {<br>                      "url": f"data:image/jpeg;base64,{base64_image}"<br>                    },<br>                  }<br>                ]<br>              }<br>            ],<br>            "max_tokens": 1000<br>        }<br><br>        response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)<br><br>        temp=response.json()<br>        return temp['choices'][0]['message']['content']</pre> <p>The question_image() function might initially seem complex, but it primarily consists of standard boilerplate code. Let’s break it down for a clearer understanding:</p> <p><strong>Function Parameters:</strong> The function accepts two key inputs:</p> <ul> <li>url: This can either be a URL to an image online or a local file path.</li> <li>query: The specific question or prompt you wish to ask about the image.</li> </ul> <p><strong>Handling Online URLs: </strong>If the url is an online URL, the function executes the code within the 'if' statement, preparing and sending the request to GPT-V.</p> <p><strong>Customization Options:</strong></p> <ul> <li>system prompt: Set within the messages parameter, this allows you to direct the model to respond in a particular manner.</li> <li>max_tokens: This can be adjusted as needed. I’ve set it to 1000 to balance detail in the response while managing token usage efficiently.</li> <li>format_response: Altering this value in the API call can guide the model to respond strictly in well-structured JSON format. You can find out about format_response and other parameters in detail <a href="https://platform.openai.com/docs/guides/text-generation/json-mode" rel="external nofollow noopener" target="_blank">here</a>.</li> </ul> <p><strong>Handling Local Files:</strong> If a local file is provided as url, the function enters the 'else' block and executes a different type of API call. The same parameters (system prompt, max_tokens, format_response) can be adjusted here as well to tailor the API's response to your needs.</p> <p>Now, all that’s required is to pass an image along with a query to perform text extraction. For demonstration purposes, I’ll use the following image of a Singapore Airlines boarding pass. While simpler than a typical receipt, it still presents similar challenges in terms of information extraction. The task will be to ask GPT-V to extract key details such as the airline name, passenger name, flight number, departure city, destination city, and the date of departure.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/760/1*yqopnaKNVxe2jwC9n2QNLA.jpeg"><figcaption>Image of a Singapore Airlines boarding pass</figcaption></figure> <pre>query="Extract the airline_name,passenger_name,flight_num,departure_city,destination_city and date_of_departure from this boarding pass in a JSON format"<br>image_url="singapore.jpg"<br>boarding_pass_json=question_image(image_url,query)<br>print(boarding_pass_json)</pre> <p>After passing the query and the image to the function, we receive the following response.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/316/1*kenfPNICrI4Qxy46VcPinQ.png"><figcaption>JSON output of the boarding pass information</figcaption></figure> <p>Pretty neat if I do say so myself.</p> <p><strong>Implementation of Azure Cognitive Services and GPT 3.5 Turbo</strong></p> <p>Let’s install all pre-requiste libraries.</p> <pre>!pip install opencv-python<br>!pip install openai</pre> <p>Note: If you are having any trouble installing opencv, refer to the official docs for clarification here: <a href="https://pypi.org/project/opencv-python/" rel="external nofollow noopener" target="_blank">opencv-python · PyPI</a></p> <p>Now let’s get all the imports out of the way.</p> <pre>import os<br>from openai import OpenAI<br>import json<br>import cv2<br>import requests<br>import time<br>os.environ['OPENAI_API_KEY']='Insert-your-openai-api-key-here'<br>client = OpenAI()</pre> <p>Setup your API key like explained above.</p> <p>First, lets define the function to perform OCR through Azure Cognitive Services.</p> <pre>subscription_key = "Insert-Your-Azure-Cognitive-Services-Key-Here"<br>def ocr(img,mime_type='image/jpeg'):<br><br>    """<br><br>    Ocr extraction from Image or Image at local path<br><br>    Input: Image or Image local path<br><br>    Output: Cognitive Services OCR output response json<br><br>    """<br>    #Change the url for your particular region. I live in India so i use the central India servers<br>    url = "https://centralindia.api.cognitive.microsoft.com/vision/v3.2/read/analyze?language=en"<br><br>    payload={}<br><br>    img_bytes=None<br><br>    if type(img)==str:<br><br>        files=[('image',('tmp.jpg',open(img,"rb"),mime_type))]<br><br>    else:<br><br>        img_bytes=img<br><br>        files=[('image',('tmp.jpg',img_bytes,mime_type))]<br><br>    headers = {'Ocp-Apim-Subscription-Key': subscription_key}<br><br>    response = requests.request("POST", url, headers=headers, data=payload, files=files, verify=False)<br><br>    status_code = response.status_code<br><br>    ocr_out=response.headers<br><br>    if status_code== 202:<br><br>        url=ocr_out['Operation-Location']<br><br>        response = requests.request("GET", url, headers=headers, data=payload, verify=False)<br><br>        ocr_out=response.json()<br><br>        while ocr_out["status"]=="running":<br><br>            time.sleep(1)<br><br>            response = requests.request("GET", url, headers=headers, data=payload, verify=False)<br><br>            ocr_out=response.json()<br><br>    #print(ocr_out)<br>    return dict(ocr_out),img_bytes</pre> <p>Firstly, ensure you have your Azure Cognitive Services vision API key ready. Input this key into the subscription_key variable. Now, let's delve into the key aspects of the ocr() function.</p> <p>The ocr() function is designed to accept two parameters:</p> <ul> <li>img: The image you wish to process. This can be either a local file path or an online image URL.</li> <li>mime_type: This has a default value set, but can be adjusted if needed.</li> </ul> <p>In most instances, you’ll provide an image URL to the ocr() function to obtain the output JSON from Azure Cognitive Services. While much of the function consists of standard boilerplate code, there is a crucial point to remember: the url variable needs to be tailored to the specific server you're using. In my case, I set it to the Central India server.</p> <p>Adjusting this variable ensures that your requests are directed to the correct Azure server, optimizing the performance and reliability of your OCR operations.</p> <p>Now let’s write the main function which incorporates the OCR output with GPT 3.5 Turbo.</p> <pre>def Image_to_JSON(image_path):<br>    # Perform OCR on the image and extract the text content<br>    result,img_bytes = ocr(image_path)<br>    data=result['analyzeResult']['readResults']<br>    texts=[line['text'] for item in data for line in item['lines']]<br><br>    # Stores the OCR content extracted from the image in a string which can be fed into ChatGPT<br>    ocr_string = " ".join(texts)<br>    ocr_string=ocr_string.lower()<br><br>    # Create a query for ChatGPT, including the OCR content and the required information<br>    ChatGPT_Query=f"""The following is a raw dump of an OCR output from a boarding pass of a flight. Extract the airline name,name of the passenger,flight number,departure city,destination city and date of departure from it. The Departure City and Destination City should be in their airport codes.<br>    The raw OCR data of boarding pass is {ocr_string}.<br>    """<br>    format_instructions="""The output should be in a JSON format where the keys should be 'airline_name','passenger_name','flight_num','departure_city','destination_city' and 'date_of_departure'."""<br>    <br>    completion = client.chat.completions.create(<br>    model="gpt-3.5-turbo-1106",<br>    messages=[<br>    {"role": "user", "content": f"{ChatGPT_Query} \n {format_instructions}"}<br>    ],<br>    response_format={ "type": "json_object" }<br>    )<br>    # Receive the JSON file<br>    data = completion.choices[0].message.content<br><br>    # Return the JSON object<br>    return json.loads(data)</pre> <p>The Image_to_JSON() function is designed with a singular purpose: to take an image path (image_path), which can be either a local file path or an online URL, and return the data contained within the image in a JSON format.</p> <p>The function begins by sending the image to Azure using the ocr() function we defined earlier. The first five lines handle this process. Once Azure processes the image, the extracted text is retrieved and stored in an array. This array is then concatenated into a single string, referred to as ocr_string, which contains all the words found in the image, converted to lowercase. The decision to use lowercase is based on preliminary observations that the system processes text more effectively in this format, though further exploration might be needed.</p> <p>Next, the query for GPT 3.5 Turbo is prepared and stored in the ChatGPT_Query variable. For demonstration purposes, I'm using the boarding pass scenario, so the query is tailored for extracting information specific to a boarding pass. However, this query can be easily modified for different information extraction tasks. The query also specifies the desired format for each piece of data.</p> <p>Another variable, format_instructions, is used to define the format in which we want the response, which in this case is JSON. This includes specifying all the keys and the types of values expected in the output.</p> <p>After sending this request to GPT 3.5 Turbo and receiving the response, we extract the JSON data as a dictionary. This dictionary is then returned to the user, who can utilize it as needed.</p> <p>It’s important to note that in the request, I have used the response_format parameter similar to what was mentioned in the implementation of Methodology 1. This approach can be adopted in the GPT-V call as well, if required.</p> <p>Now let’s test it out on the same boarding pass image I used for testing methodology 1.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/760/1*yqopnaKNVxe2jwC9n2QNLA.jpeg"><figcaption>Image of a Singapore Airlines boarding pass</figcaption></figure> <pre>b=Image_to_JSON(r"singapore.jpg")<br>import pprint #Just for demonstration purposes, can just use regular print<br>pprint.pprint(b)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/302/1*vGJl68NRgF_FclH8haCK8w.png"><figcaption>JSON output of Boarding pass through ACS and GPT 3.5</figcaption></figure> <p>As you can see, it works perfectly.</p> <p><strong>Implementation of PaddleOCR and Zephyr-7B</strong></p> <p>Let’s install all pre-requiste libraries for Zephyr-7B.</p> <pre>!pip install git+https://github.com/huggingface/transformers.git<br>!pip install accelerate</pre> <p>Pre-requisite libraries for PaddleOCR</p> <pre>!git clone https://github.com/PaddlePaddle/PaddleOCR.git<br>!python3 -m pip install paddlepaddle-gpu<br>!pip install "paddleocr&gt;=2.0.1"</pre> <p>Now let’s get all the imports out of the way.</p> <pre>import torch<br>from paddleocr import PaddleOCR<br>from transformers import pipeline</pre> <p>Now let’s write the OCR through PaddleOCR first.</p> <pre>ocr = PaddleOCR(use_angle_cls=True, lang='en',use_space_char=True,show_log=False,enable_mkldnn=True)<br><br>img_path = 'singapore.jpg'<br>result = ocr.ocr(img_path, cls=True)<br><br>ocr_string = ""  <br>#Extract the text from the OCR result and concatenate it to ocr_string<br>for i in range(len(result[0])):<br>    ocr_string = ocr_string + result[0][i][1][0] + " "</pre> <p>In this part of the process, we initialize PaddleOCR and load it into the ocr variable, configuring it with several parameters. While many of these parameters, like use_angle_cls and use_space_char, are standard, I've included an additional parameter named enable_mkldnn. This particular parameter enhances performance with minimal overhead, essentially providing a free performance boost. For a more detailed understanding of what each parameter does, I recommend consulting the <a href="https://github.com/PaddlePaddle/PaddleOCR" rel="external nofollow noopener" target="_blank">PaddleOCR documentation</a>.</p> <p>After setting up PaddleOCR, we proceed to pass the image path, stored in the img_path variable, to it. This image path is the same Singapore Airlines boarding pass that we used in the previous methodology. The OCR then processes the image, and we compile all the detected text into a single variable called ocr_string, similar to what was done in Methodology 2.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VJfHz3ZEa7p7iMXRnoK7gA.png"><figcaption>Sample Output of ocr_string</figcaption></figure> <p>Now let’s move onto the LLM code.</p> <pre>pipe = pipeline("text-generation", model="HuggingFaceH4/zephyr-7b-alpha", torch_dtype=torch.bfloat16, device_map="auto")</pre> <p>We use the pipeline from the Transformers library to download and use zephyr-7b-alpha model from HuggingFace.</p> <pre># Each message can have 1 of 3 roles: "system" (to provide initial instructions), "user", or "assistant". For inference, make sure "user" is the role in the final message.<br>messages = [<br>    {<br>        "role": "system",<br>        "content": "You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.",<br>    },<br>    {"role": "user", "content": f"Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: {ocr_string}"},<br>]<br># We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating<br>prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)</pre> <p>In this step, we craft a prompt tailored to our specific use case. For this demonstration, which focuses on boarding passes, the prompts are customized accordingly. However, it’s important to note that these prompts can be modified to suit a wide range of other use cases. Once the prompt is written, we utilize the pipeline class, which we defined earlier, to format our message into a structure that Zephyr-7B-alpha can interpret effectively. The resulting format of the prompt variable is then ready for processing.</p> <p>Here is how prompt looks after it has been formatted.</p> <pre>&lt;|system|&gt;<br>You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.&lt;/s&gt;<br>&lt;|user|&gt;<br>Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: THE PRIVATE ROOM SINGAPORE SUITES SUITES HENG KOK HONGMR HENG KOK HONG MR SQ*G Fron SINGAPORE TOSYD Date19JAN18 To SYDNEY Flight SQ 231 FronSIN Sulte sutte Boarding Group FlightSQ231 Boarding time 3F Terminal Gate 3F 3 12:15AM Date 19JAN18 19JAN18 00325 ETNo YOU ARE INVITED TO THE PRIVATE ROOM SKL GATE CLOSES 10 MINS BEFORE DEPARTURE A STAR ALLIANCEMEMBER 00325 ETNO&lt;/s&gt;<br>&lt;|assistant|&gt;</pre> <p>The next stage involves sending the formatted prompt to the Zephyr-7B model for information extraction.</p> <pre>outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)<br>print(outputs[0]["generated_text"])</pre> <p>Key variables in this process include max_new_tokens, which controls the number of new tokens the model can generate. A higher value in this variable allows for greater creativity and length in the message produced by the model. The variables temperature, top_k, and top_p are used to regulate the randomness of the model's responses.</p> <ul> <li>temperature (float, optional, defaults to 1.0): This parameter controls the randomness of predictions by scaling the logits before applying softmax. <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation" rel="external nofollow noopener" target="_blank">A higher value results in more random completions, while a lower value makes the model more confident but also more conservative</a>.</li> <li> <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation" rel="external nofollow noopener" target="_blank">top_k (int, optional, defaults to 50): This is the number of highest probability vocabulary tokens to keep for top-k-filtering</a>. <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation" rel="external nofollow noopener" target="_blank">During the sampling process, it restricts the pool of words to select from to the top k words</a>.</li> <li>top_p (float, optional, defaults to 1.0): Also known as nucleus sampling, it is an alternative to Top-K sampling. <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation" rel="external nofollow noopener" target="_blank">Instead of sampling only from the most likely K words, in Top-P sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p</a>.</li> </ul> <p>You can find more detailed information about these parameters and their usage in the <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation" rel="external nofollow noopener" target="_blank">Hugging Face documentation</a> and <a href="https://discuss.huggingface.co/t/order-of-execution-of-top-k-top-p-sampling-along-with-temperature/55569" rel="external nofollow noopener" target="_blank">this discussion on Hugging Face’s forum</a>. You can also find examples of their usage in this <a href="https://gist.github.com/MarcSkovMadsen/eae998fbcb299fae9e92ab0089e7eff8" rel="external nofollow noopener" target="_blank">GitHub Gist</a>.</p> <p>Once the model has processed the prompt, we then extract and print the JSON output. This output contains all the relevant information extracted from the image.</p> <p>This is how the entire end-to-end function will look.</p> <pre>def Image_to_JSON(image_path):<br>    # Perform OCR on the image and extract the text content<br>    result = ocr.ocr(image_path, cls=True)<br><br>    ocr_string = ""  # Stores the OCR content extracted from the image in a string which can be fed into ChatGPT<br><br>    # Extract the text from the OCR result and concatenate it to ocr_string<br>    for i in range(len(result[0])):<br>        ocr_string = ocr_string + result[0][i][1][0] + " "<br><br>    messages = [<br>    {<br>        "role": "system",<br>        "content": "You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.",<br>    },<br>    {"role": "user", "content": f"Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: {ocr_string}"},<br>    ]<br>    # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating<br>    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)<br>    outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)<br>    print(outputs[0]["generated_text"])<br>    return outputs[0]["generated_text"]</pre> <p>Here’s an overview of how the entire Image_to_JSON() function operates:</p> <ul> <li>The function accepts an image URL as input.</li> <li>It processes this input to return a JSON output generated by Zephyr-7B-alpha.</li> </ul> <p>Let’s apply this function to the same Singapore Airlines boarding pass used earlier.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/760/1*yqopnaKNVxe2jwC9n2QNLA.jpeg"><figcaption>Image of a Singapore Airlines boarding pass</figcaption></figure> <pre>&lt;|system|&gt;<br>You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.&lt;/s&gt;<br>&lt;|user|&gt;<br>Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: THE PRIVATE ROOM SINGAPORE SUITES SUITES HENG KOK HONGMR HENG KOK HONG MR SQ*G Fron SINGAPORE TOSYD Date19JAN18 To SYDNEY Flight SQ 231 FronSIN Sulte sutte Boarding Group FlightSQ231 Boarding time 3F Terminal Gate 3F 3 12:15AM Date 19JAN18 19JAN18 00325 ETNo YOU ARE INVITED TO THE PRIVATE ROOM SKL GATE CLOSES 10 MINS BEFORE DEPARTURE A STAR ALLIANCEMEMBER 00325 ETNO&lt;/s&gt;<br>&lt;|assistant|&gt;<br>Here's the JSON output:<br><br>```json<br>{<br>  "Passenger": {<br>    "Name": "HENG KOK HONG MR"<br>  },<br>  "Airlines": {<br>    "Name": "SQ"<br>  },<br>  "Flight": {<br>    "Number": "SQ 231",<br>    "Departure": {<br>      "City": "SINGAPORE",<br>      "Date": "19JAN18",<br>      "Time": "12:15AM"<br>    },<br>    "Arrival": {<br>      "City": "SYDNEY"<br>    },<br>    "Boarding": {<br>      "Group": "3",<br>      "Time": "3F Terminal Gate 3F"<br>    }<br>  },<br>  "Other Information": [<br>    {<br>      "Title": "The Private Room Singapore Suites",<br>      "Location": "Heng Kok Hong MR"<br>    },<br>    {<br>      "Title": "Star Alliance Member",<br>      "ETNo": "00325"<br>    },<br>    {<br>      "Title": "Gate Closes",<br>      "Time": "10 Mins Before Departure",<br>      "Location": "SKL Gate"<br>    }<br>  ]<br>}<br>```<br><br>Note: The "Other Information" section has been added to contain any other details that may be relevant to the passenger's travel.</pre> <p>The results demonstrate that the function is quite effective, and with some fine-tuning of the prompts, it can be reliably used for extracting information from images.</p> <p><strong>Points to consider</strong></p> <p>As we wrap up the implementations, it’s essential to understand the underlying logic: essentially, any OCR can be paired with any Large Language Model (LLM) to parse and structure data. While I demonstrated these methodologies using popular tools like Azure Cognitive Services, PaddleOCR, GPT 3.5 Turbo, and Zephyr-7B, the concepts are not limited to these specific tools. They serve as examples to illustrate the process, but you’re encouraged to explore other OCR and LLM options that might better suit your specific needs or preferences.</p> <p>Furthermore, if you’re willing to invest additional effort into refining your data extraction process, consider fine-tuning your chosen LLM. Methods such as <a href="https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b" rel="external nofollow noopener" target="_blank">Low Rank Adaptation (LoRA)</a> can be highly effective. By training the LLM on a series of prompt-output pairs consisting of Raw OCR dumps and the output being how you desire the OCR to be formatted, you can significantly enhance its ability to parse OCR dumps and generate more consistent JSON outputs. This fine-tuning can tailor the model to be more adept at understanding and structuring the specific types of data you’re working with.</p> <p>Infact, you could use Methodology 2 of using Azure Cognitive Services and GPT 3.5 Turbo to generate these prompt-output pairs which you could use to fine-tune the LLM of your choice to eventually switch to Methodology 3.</p> <p>For those interested in exploring an approach similar to Methodology 1, you might want to investigate open-source Visual Question Answering models like <a href="https://llava-vl.github.io/" rel="external nofollow noopener" target="_blank">LLaVA 1.5</a> or <a href="https://huggingface.co/adept/fuyu-8b" rel="external nofollow noopener" target="_blank">Fuyu 8B</a>. These models offer functionalities akin to GPT-V and can be a great alternative, especially if you’re looking for open-source options. Implementing your solution with these models can provide you with a similar experience to GPT-V, potentially with the added benefits of customization and greater control over your data processing pipeline.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ASXh8eBF56J7ZenrgzIe3Q.png"><figcaption>Example demonstrating the capabilities of LLaVa. Source: <a href="https://llava-vl.github.io/" rel="external nofollow noopener" target="_blank">LLaVA (llava-vl.github.io)</a></figcaption></figure> <p><strong>Conclusion</strong></p> <p>In conclusion, this project showcases the dynamic and evolving landscape of data extraction from images, demonstrating the effectiveness of combining OCR with Large Language Models. Whether through the simplicity and power of GPT-4 Vision, the precision of Azure Cognitive Services paired with GPT 3.5 Turbo, or the flexibility and security of PaddleOCR with Zephyr-7B, each methodology presents its unique advantages and potential applications.</p> <p>The key takeaway is the versatility and adaptability of these approaches. By understanding the principles behind each method, you can mix and match different OCR and LLM tools to suit your specific needs, even venturing into the realm of fine-tuning models for enhanced performance.</p> <p>This project is just a glimpse into that potential, encouraging further exploration and experimentation in the fascinating realm of visual data, text recognition, and language understanding.</p> <h3>Follow For More!</h3> <p><em>I try to implement a lot of theoretical concepts in the ML space, with an emphasis on practical and intuitive applications.</em></p> <p><em>Thanks for reading this article! If you have any questions, I will be happy to answer them. Feel free to message me on my </em><a href="http://linkedin.com/in/suresh-raghu" rel="external nofollow noopener" target="_blank"><em>LinkedIn</em></a><em> or my </em><a href="http://sureshraghu0706@gmail.com/" rel="external nofollow noopener" target="_blank"><em>email</em></a><em> for other queries.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0c07754813cc" width="1" height="1" alt=""></p> </body></html>