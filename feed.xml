<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://r-suresh07.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://r-suresh07.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-03T17:45:15+00:00</updated><id>https://r-suresh07.github.io/feed.xml</id><title type="html">R. Suresh</title><subtitle>Lead AI Engineer building production-scale AI systems and researching uncertainty quantification in vision-language models. </subtitle><entry><title type="html">No-Nonsense Guide to LangChain</title><link href="https://r-suresh07.github.io/blog/2024/no-nonsense-guide-to-langchain/" rel="alternate" type="text/html" title="No-Nonsense Guide to LangChain"/><published>2024-03-16T16:48:03+00:00</published><updated>2024-03-16T16:48:03+00:00</updated><id>https://r-suresh07.github.io/blog/2024/no-nonsense-guide-to-langchain</id><content type="html" xml:base="https://r-suresh07.github.io/blog/2024/no-nonsense-guide-to-langchain/"><![CDATA[<p>How to get started with LangChain without the pain of navigating the docs.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/451/1*bbC8-9DI2kMF3j5eKoG4kg.png"/></figure> <p>Have you been wanting to work with Large Language Models (LLMs) but find that each LLM API service has different parameters and different JSON returns? Does prototyping become cumbersome because you need to rewrite a lot of code anytime you want to try a different model? Or, do you find that writing your own routing logic for multiple chained prompts becomes too complicated too fast? You need to ensure the correct order of execution, proper batching, and potentially even the headache of asynchronicity. Maybe you’re building a chatbot and find managing all the messages and parameters irritating. The good news is that LangChain solves a lot of these issues!</p> <p>LangChain is an open-source framework designed to make working with LLMs easier, providing a standard interface across a wide variety of models. It abstracts away the complexities of each model and allows us to do rapid prototyping.</p> <p>Note: While LangChain excels at interacting with cloud-based LLMs like ChatGPT, Claude, and Gemini, it might not be the best choice for local LLMs. For local environments, consider services like Ollama or LM Studio, which offer finer control over these models. Specifically in the case of LM Studio, you can expose an API endpoint on your machine and then modify the base url parameter in the OpenAI API and you can re-use the same code that you use when dealing with an OpenAI API. You can learn more about Ollama <a href="https://ollama.com/">here</a> and LM Studio <a href="https://medium.com/@ingridwickstevens/running-a-local-openai-compatible-mixtral-server-with-lm-studio-cd602efbf808">here</a>.</p> <p>In this article, I’ll break down the fundamental components of LangChain, demonstrate how to combine them for basic chains, explore the combination of multiple chains, discuss asynchronous execution, and present a project that applies all these principles.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/704/1*i5U5ogIcC6AVGv6DossPCg.png"/><figcaption>Basic Parts of LangChain</figcaption></figure> <p><strong>Basic parts of Chain:</strong></p> <p>LangChain ‘chains’ are the core of its functionality. A chain handles the execution of a single prompt. While chains might seem like overkill for a simple one-prompt task, the real power comes from linking multiple prompts together. With a single command, you can effortlessly execute a sequence of prompts and store their results.</p> <p><strong>Important Note:</strong> If you’re referencing the LangChain docs, be aware that some parts of the docs may use the older, deprecated chain system. Things like SequentialChain are no longer necessary. Instead, focus on using the <a href="https://python.langchain.com/docs/expression_language/">LangChain Expression Language (LCEL)</a>. LCEL is designed to make building and understanding pipelines even easier.</p> <ol><li><strong>Prompts</strong></li></ol> <p>This is pretty self-explanatory. Prompts are the instructions you give the Large Language Model (LLM). LangChain uses ‘Templates’ to make prompts more flexible.</p> <pre>from langchain.prompts import PromptTemplate<br /><br />content_template=&quot;&quot;&quot;# CONTEXT #<br />I want to advertise for my company&#39;s duty free product catalogue. The SKU Name of the product is {sku_name}, brand of the product is {brand_name} and its category is : {sub_category}.<br /><br /># OBJECTIVE #<br />Create new product description for the above product based on the following existing description: {description}. Extract all key points and features from the existing description and write a new description from them. The word count should be similar to the original description.<br /><br /># STYLE #<br />The writing style needs focus on the unique aspects of the product - its ingredients, its process, its history. Use descriptive language to evoke sensory experience - taste, smell, touch. Be precise and concise when writing. The content needs to be simple enough for the common man to understand it.<br />Following words are BLACKLISTED from appearing in the response: &#39;transformative&#39;,&#39;tapestry&#39;,&#39;like&#39;,&#39;;&#39;,&#39;-&#39;.<br />REPLACE the following words with a more descriptive and specific response: &#39;foster&#39;,&#39;fostering&#39;,&#39;all about&#39;,&#39;is about&#39;,&#39;think of&#39;,&#39;like&#39;,&#39;but also&#39;.<br /><br /># TONE #<br />Simple, clear and elegant.<br /><br /># AUDIENCE #<br />My company&#39;s audience profile is the common man who likes to indulge in the finer things in life every once in a while.<br />&quot;&quot;&quot;<br /><br />content_prompt=PromptTemplate(input_variables=[&quot;sku_name&quot;,&quot;brand_name&quot;,&quot;sub_category&quot;,&quot;description&quot;],template=content_template)</pre> <p>Above is an example of a prompt that I used in a recent project. You basically create a python string with the prompt you want the LLM to process, and wherever you want to dynamically insert a variable like different product names, product categories, product subcategories and so on, you can use the {} notation similar to a python f-string. Then all you need to do is initialize the PromptTemplate class with all the variables that you declared.</p> <p>If you are wondering why I have formatted the prompt in a bizarre way, it is a prompt engineering framework called CO-STAR which was designed by the GovTech Singapore’s AI and Data Science team which allows us to guide an LLM model easily by just following a few guidelines. More information about CO-STAR <a href="https://medium.com/towards-data-science/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41">here</a>.</p> <p><strong>Note:</strong> If you are dealing with Chatbots, you can use a special template called ChatPromptTemplate which will help you deal with features specific to Chat models. This guide won’t be dealing with Chatbot specific code but I will provide references where you can find out more about making a Chatbot through LangChain <a href="https://python.langchain.com/docs/use_cases/chatbots/quickstart">here</a>.</p> <p><strong>2. Models</strong></p> <p>LangChain uses various model classes to interface with various types of LLMs. Some notable model classes are Anthropic , AzureOpenAI , OpenAI etc. You can find all the supported models <a href="https://python.langchain.com/docs/integrations/llms/">here</a>.</p> <pre>llm=ChatOpenAI(<br />openai_api_key=&quot;replace-this-with-your-api-key&quot;,<br />model=&quot;gpt-4-1106-preview&quot;,<br />model_kwargs={<br />    &quot;response_format&quot;: { <br />      &quot;type&quot;: &quot;json_object&quot; <br />    },<br />    &quot;presence_penalty&quot;:1.0<br />  },<br />temperature=1<br />)</pre> <p>You can see above how you can declare a model object for OpenAI models using ChatOpenAI . Some things to note:</p> <ul><li>model_kwargs is used to declare model specific parameters that may not be common to other models. For example, the gpt-4-1106-preview API has a parameter called response_format which is used to force the API to generate valid JSON schema. This parameter is unique to this model, hence we use model_kwargs to specify it. Another unique parameter to the gpt-4-1106-preview model is something known as presence_penalty . This is used to reduce the repetition of words or phrases generated by the model.</li><li>temperature isn’t declared inside model_kwargs because it’s a very common parameter across multiple model providers and hence can be a global parameter.</li></ul> <p><strong>3. Output Parsers</strong></p> <p>The final component of a LangChain chain is an output parser. When integrating LLM data into a production pipeline, you need a consistent way to retrieve the output. This output might be in JSON, XML, or other formats. The goal is to extract the relevant information reliably. However, this can be challenging because LLMs sometimes include extraneous text along with a valid JSON structure.</p> <p><strong>Example:</strong></p> <p>Consider this LLM response:</p> <pre>Sure! The output in JSON format is below:<br />{<br />  &quot;product_info&quot;:&quot;insert product info&quot;<br />}</pre> <p>The response mixes introductory text with the JSON data. You can’t parse this directly; you need to find and extract the JSON portion first. Fortunately, LangChain offers several output parsers designed to handle these situations. One of the most useful and flexible parsers is the Pydantic parser. It allows us to define our own object schema, ensuring our LLM output is structured as expected. Let’s see how a Pydantic parser works, continuing with the prompt we started earlier:</p> <pre>from langchain.output_parsers import PydanticOutputParser<br />from langchain.prompts import PromptTemplate<br />from langchain_core.pydantic_v1 import BaseModel, Field, validator<br /><br />class Product(BaseModel):<br />    product_description : str = Field(description=&quot;The generated product description&quot;)<br /><br />parser= PydanticOutputParser(pydantic_object=Product)<br /><br />content_template=&quot;&quot;&quot;# CONTEXT #<br />I want to advertise for my company&#39;s duty free product catalogue. The SKU Name of the product is {sku_name}, brand of the product is {brand_name} and its category is : {sub_category}.<br /><br /># OBJECTIVE #<br />Create new product description for the above product based on the following existing description: {description}. Extract all key points and features from the existing description and write a new description from them. The word count should be similar to the original description.<br /><br /># STYLE #<br />The writing style needs focus on the unique aspects of the product - its ingredients, its process, its history. Use descriptive language to evoke sensory experience - taste, smell, touch. Be precise and concise when writing. The content needs to be simple enough for the common man to understand it.<br />Following words are BLACKLISTED from appearing in the response: &#39;transformative&#39;,&#39;tapestry&#39;,&#39;like&#39;,&#39;;&#39;,&#39;-&#39;.<br />REPLACE the following words with a more descriptive and specific response: &#39;foster&#39;,&#39;fostering&#39;,&#39;all about&#39;,&#39;is about&#39;,&#39;think of&#39;,&#39;like&#39;,&#39;but also&#39;.<br /><br /># TONE #<br />Simple, clear and elegant.<br /><br /># AUDIENCE #<br />My company&#39;s audience profile is the common man who likes to indulge in the finer things in life every once in a while.<br /><br /># RESPONSE #<br />{format_instructions}<br />&quot;&quot;&quot;<br /><br />content_prompt=PromptTemplate(input_variables=[&quot;sku_name&quot;,&quot;brand_name&quot;,&quot;sub_category&quot;,&quot;description&quot;],template=content_template,partial_variables={&quot;format_instructions&quot;:parser.get_format_instructions()})</pre> <p><strong>Explanation</strong></p> <ul><li>We define a class (Product) to represent the desired output structure. The description parameter in the Field object is crucial as it tells the LLM what kind of text should be captured by the product_description object.</li><li>Custom validator functions can be added to the class for additional output validation (refer to the <a href="https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic">LangChain docs</a> for details).</li><li>Our prompt remains mostly unchanged, except we add the format_instructions variable. The partial_variables in our PromptTemplate will include the format instructions generated from our Product class.</li></ul> <p>If you were curious as to what the get_format_instructions() looks like, here you go.</p> <pre>The output should be formatted as a JSON instance that conforms to the JSON schema below.<br /><br />As an example, for the schema {&quot;properties&quot;: {&quot;foo&quot;: {&quot;title&quot;: &quot;Foo&quot;, &quot;description&quot;: &quot;a list of strings&quot;, &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;string&quot;}}}, &quot;required&quot;: [&quot;foo&quot;]}<br />the object {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]} is a well-formatted instance of the schema. The object {&quot;properties&quot;: {&quot;foo&quot;: [&quot;bar&quot;, &quot;baz&quot;]}} is not well-formatted.<br /><br />Here is the output schema:<br />```<br />{&quot;properties&quot;: {&quot;product_description&quot;: {&quot;title&quot;: &quot;Product Description&quot;, &quot;description&quot;: &quot;The generated product description&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;product_description&quot;]}<br />```</pre> <p>We can see how LangChain has abstracted the tedious part of prompt engineering and made it easier to get consistent outputs.</p> <p>Getting consistent output is one of the many uses of parsers, another major use case is to automatically fix any errors encountered by the model. For example, LLMs aren’t perfect and make mistakes like not generating a valid JSON schema. We don’t want the process to break during the pipeline so we can mitigate this issue by using the OutputFixingParser which takes an existing parser as input along with an LLM to fix any potential mistakes that we might encounter. All you need to change in the existing code is the following:</p> <pre>from langchain.output_parsers import OutputFixingParser<br />from langchain.output_parsers import PydanticOutputParser<br />from langchain.prompts import PromptTemplate<br />from langchain_core.pydantic_v1 import BaseModel, Field, validator<br /><br />class Product(BaseModel):<br />    product_description : str = Field(description=&quot;The generated product description&quot;)<br /><br />parser= PydanticOutputParser(pydantic_object=Product)<br /><br />output_fixing_parser=OutputFixingParser(parser=parser,llm=llm)<br /><br />content_template=&quot;&quot;&quot;# CONTEXT #<br />I want to advertise for my company&#39;s duty free product catalogue. The SKU Name of the product is {sku_name}, brand of the product is {brand_name} and its category is : {sub_category}.<br /><br /># OBJECTIVE #<br />Create new product description for the above product based on the following existing description: {description}. Extract all key points and features from the existing description and write a new description from them. The word count should be similar to the original description.<br /><br /># STYLE #<br />The writing style needs focus on the unique aspects of the product - its ingredients, its process, its history. Use descriptive language to evoke sensory experience - taste, smell, touch. Be precise and concise when writing. The content needs to be simple enough for the common man to understand it.<br />Following words are BLACKLISTED from appearing in the response: &#39;transformative&#39;,&#39;tapestry&#39;,&#39;like&#39;,&#39;;&#39;,&#39;-&#39;.<br />REPLACE the following words with a more descriptive and specific response: &#39;foster&#39;,&#39;fostering&#39;,&#39;all about&#39;,&#39;is about&#39;,&#39;think of&#39;,&#39;like&#39;,&#39;but also&#39;.<br /><br /># TONE #<br />Simple, clear and elegant.<br /><br /># AUDIENCE #<br />My company&#39;s audience profile is the common man who likes to indulge in the finer things in life every once in a while.<br /><br /># RESPONSE #<br />{format_instructions}<br />&quot;&quot;&quot;<br /><br />content_prompt=PromptTemplate(input_variables=[&quot;sku_name&quot;,&quot;brand_name&quot;,&quot;sub_category&quot;,&quot;description&quot;],template=content_template,partial_variables={&quot;format_instructions&quot;:parser.get_format_instructions()})</pre> <p>All we needed to do is to wrap our existing parser with the new OutputFixingParser which will ensure that we don’t get any unexpected issues when running the pipeline.</p> <p><strong>Putting it together:</strong></p> <p>Now that we have seen how individual components work, let’s put them all together and create our first program! The entire code is now below:</p> <pre>from langchain.chat_models import ChatOpenAI<br />from langchain.prompts import PromptTemplate<br />from langchain.output_parsers import PydanticOutputParser<br />from langchain_core.pydantic_v1 import BaseModel, Field<br />from langchain.output_parsers import OutputFixingParser<br /><br />llm=ChatOpenAI(<br />    openai_api_key=&quot;insert-your-api-key-here&quot;,<br />    model=&quot;gpt-4-1106-preview&quot;,<br />    model_kwargs={<br />    &quot;presence_penalty&quot;:1.0<br />    },<br />    temperature=1<br />    )<br /><br />class Product(BaseModel):<br />    product_description : str = Field(description=&quot;The generated product description&quot;)<br /><br />content_template=&quot;&quot;&quot;# CONTEXT #<br />I want to advertise for my company&#39;s duty free product catalogue. The SKU Name of the product is {sku_name}, brand of the product is {brand_name} and its category is : {sub_category}.<br /><br /># OBJECTIVE #<br />Create new product description for the above product based on the following existing description: {description}. Extract all key points and features from the existing description and write a new description from them. The word count should be similar to the original description.<br /><br /># STYLE #<br />The writing style needs focus on the unique aspects of the product - its ingredients, its process, its history. Use descriptive language to evoke sensory experience - taste, smell, touch. Be precise and concise when writing. The content needs to be simple enough for the common man to understand it.<br />Following words are BLACKLISTED from appearing in the response: &#39;transformative&#39;,&#39;tapestry&#39;,&#39;like&#39;,&#39;;&#39;,&#39;-&#39;.<br />REPLACE the following words with a more descriptive and specific response: &#39;foster&#39;,&#39;fostering&#39;,&#39;all about&#39;,&#39;is about&#39;,&#39;think of&#39;,&#39;like&#39;,&#39;but also&#39;.<br /><br /># TONE #<br />Simple, clear and elegant.<br /><br /># AUDIENCE #<br />My company&#39;s audience profile is the common man who likes to indulge in the finer things in life every once in a while.<br /><br />{format_instructions}<br />&quot;&quot;&quot;<br /><br />parser= PydanticOutputParser(pydantic_object=Product)<br />output_fixing_parser=OutputFixingParser(parser=parser,llm=llm)<br />content_prompt=PromptTemplate(input_variables=[&quot;sku_name&quot;,&quot;brand_name&quot;,&quot;sub_category&quot;,&quot;description&quot;],template=content_template,partial_variables={&quot;format_instructions&quot;:parser.get_format_instructions()})<br />content_runnable = content_prompt | llm  | output_fixing_parser</pre> <p>Viola! Our prompt is ready to be run. You might have noticed something I haven’t mentioned before, what&#39;s a ‘runnable’ and what are those ‘|’ operators doing?</p> <p>A runnable can basically be considered as a series of operations that need to be performed in a row. You can also call it a “chain”.</p> <p>We are basically feeding the content_prompt template into the GPT-4 Turbo API which is then fed into the output_fixing_parser which finally gives us a structured response containing the information we need. If you have ever used Bash or other Linux based terminals you should be familiar with the ‘|’ (pipe) operator and how it is used to feed output of one command into another command. This is the power of the LangChain Expression Language, it is very easy to visualize all of the runnables.</p> <p>Now let’s try running the runnable with the following sample input.</p> <pre>&quot;brand_name&quot;:&quot;Chloe&quot;<br />&quot;sku_name&quot;:&quot;CHLOÉ Atelier des Fleurs Magnolia Alba Eau de Parfum 150ml&quot;<br />&quot;sub_category&quot;:&quot;Perfumes&quot;<br />&quot;description&quot;:&quot;The House of Chloé unveils a collection of nine exclusive Eau de Parfum fragrances for woman: Atelier des Fleurs.\nMagnolia Alba transcribes the smooth, plump and slightly lemony notes of magnolia blossoms in spring.&quot;</pre> <pre>user_input={<br />&quot;brand_name&quot;:&quot;Chloe&quot;,<br />&quot;sku_name&quot;:&quot;CHLOÉ Atelier des Fleurs Magnolia Alba Eau de Parfum 150ml&quot;,<br />&quot;sub_category&quot;:&quot;Perfumes&quot;,<br />&quot;description&quot;:&quot;The House of Chloé unveils a collection of nine exclusive Eau de Parfum fragrances for woman: Atelier des Fleurs.\nMagnolia Alba transcribes the smooth, plump and slightly lemony notes of magnolia blossoms in spring.&quot;<br />}<br /><br />result=content_runnable.invoke(user_input)<br />print(result.product_description)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/969/1*zCOogWCjIzk1kM_Zo14vrQ.png"/><figcaption>Output</figcaption></figure> <p>We can execute a runnable by using the .invoke() method. The invoke() method accepts a dictionary consisting of all the input_variables that we declared in the content_prompt variable. Note: We don’t need to provide partial_variables dynamically.</p> <p>If everything goes right, you should get your result in the proper format which you specified.</p> <p>Now, you might be wondering, why go through all this effort just to get an output for a single prompt? Well, the magic of LangChain shines when you’ve set up the proper foundation. Let’s say you want to try a different LLM like Gemini-pro. All you need to do is replace:</p> <pre>llm=ChatOpenAI(<br />    openai_api_key=&quot;insert-your-api-key-here&quot;,<br />    model=&quot;gpt-4-1106-preview&quot;,<br />    model_kwargs={<br />    &quot;presence_penalty&quot;:1.0<br />    },<br />    temperature=1<br />    )</pre> <p>with</p> <pre># You might have to install a dependency for this, refer to the ChatGoogleGenerativeAI page on LangChain Docs for more<br />from langchain_google_genai import ChatGoogleGenerativeAI<br /><br />if &quot;GOOGLE_API_KEY&quot; not in os.environ:<br />    os.environ[&quot;GOOGLE_API_KEY&quot;] = &quot;insert-your-api-key-here&quot;<br /><br />llm = ChatGoogleGenerativeAI(model=&quot;gemini-pro&quot;)</pre> <p>And that’s it! The rest of your code remains unchanged, allowing for super-fast prototyping and easy comparisons between different models.</p> <p>Let’s say you want to run multiple inputs through your runnable. LangChain makes this easy! Every runnable object has attributes like sync(), batch(), async(), abatch(), and more. Refer to the <a href="https://python.langchain.com/docs/expression_language/">LCEL page</a> if you wish to learn more. Here&#39;s how to perform batching in LangChain:</p> <pre>array_of_input=[<br />    {&#39;brand_name&#39;: &#39;Tiffani &amp; Co.&#39;,<br />    &#39;sku_name&#39;: &#39;TIFFANY &amp; CO. Sheer Eau de Toilette 75ml&#39;,<br />    &#39;sub_category&#39;: &#39;Perfumes&#39;,<br />    &#39;description&#39;: &#39;A bright and sparkling interpretation of the signature fragrance, Tiffany Sheer Eau de Toilette is a vibrant floral scent with notes of ylang ylang, black currant and the noble iris.&#39;,<br />    &#39;rewritten&#39;: None},<br />    {&#39;brand_name&#39;: &#39;Davidoff&#39;,<br />    &#39;sku_name&#39;: &#39;Davidoff Cool Water After Shave Lotion for Him 75ml&#39;,<br />    &#39;sub_category&#39;: &#39;Perfumes&#39;,<br />    &#39;description&#39;: &quot;Step up your grooming routine with Davidoff&#39;s Cool Water After Shave Lotion. This Lotion nourishes, soothes and calms your skin post-shave, leaving it lightly scented with the marine notes of Cool Water. Cool Water After Shave Lotion, the ultimate touch to complement your Eau de Toilette.&quot;,<br />    &#39;rewritten&#39;: None},<br />    {&#39;brand_name&#39;: &#39;Davidoff&#39;,<br />    &#39;sku_name&#39;: &quot;Davidoff Men&#39;s 3-Pc. Cool Water Eau de Toilette Festive Gift Set&quot;,<br />    &#39;sub_category&#39;: &#39;Perfumes&#39;,<br />    &#39;description&#39;: &quot;This Christmas, celebrate a festive moment with the Cool Water Eau de Toilette Festive Gift Set. Dive deep into the fresh invigorating power of the ocean and pure masculinity of the iconic Cool Water Eau de Toilette fragrance. Bringing together the essentials of a man&#39;s routine, Cool Water Man Eau de Toilette, a foaming Shower Gel hair and body wash that cleanses and refreshes while protecting the skin’s natural balance and an After Shave Balm that immediately nourishes, soothes and calms freshly shaven skin. Let the iconic fougere aromatic scent of Cool Water carry you away, feel the strength of deep waters and pure masculinity envelop you. This festive gift set includes Davidoff Cool Water Man Eau de Toilette 125 ml, Shower Gel 75 ml and After Shave Balm 75 ml.&quot;,<br />    &#39;rewritten&#39;: None}<br />    ]<br /><br />result=content_runnable.batch(array_of_input)<br />print(result)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/930/1*0AEU9PYFgiHPbQob3vNzXg.png"/><figcaption>Batching in LangChain</figcaption></figure> <p>To perform batching, create an array of dictionaries, each containing the necessary input data for your runnable. Then, simply pass this array to the batch() function. LangChain will process all inputs in parallel for maximum efficiency. You will then receive an array of Product objects containing all the information you need.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/829/1*NeUeh8UGx7sXSDaGEqQp4g.png"/><figcaption>Comparison between linear execution and batched execution</figcaption></figure> <p><strong>Routing in LangChain</strong></p> <p>Let’s do something a bit more complex. We’ll build a system that automatically routes customer support queries to the appropriate agent based on their category (payment, delivery, or product). This will be accomplished using LangChain’s RunnableLambda class.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/975/1*Z-bTIeqpFcC9nF0XPvkPZA.png"/><figcaption>High Level Overview of the system</figcaption></figure> <p>Let’s create a classifier to determine if a query relates to payments, delivery, or product issues.</p> <pre>from langchain.chat_models import ChatOpenAI<br />from langchain.prompts import PromptTemplate<br />from langchain.output_parsers import PydanticOutputParser<br />from langchain_core.pydantic_v1 import BaseModel, Field<br />from langchain.output_parsers import OutputFixingParser<br />from langchain_core.runnables import RunnableLambda<br /><br />llm=ChatOpenAI(openai_api_key=&quot;insert-api-key-here&quot;,model=&quot;gpt-4-1106-preview&quot;,model_kwargs={<br />    &quot;presence_penalty&quot;:1.0<br />  },temperature=1)<br /><br />class Query(BaseModel):<br />    query_category : str = Field(description=&quot;The query category to be classified&quot;) <br /><br />query_classifier_template=&quot;&quot;&quot;<br />Your job is to classify user queries into payment issues, delivery issues and product issues. <br /><br />Do NOT respond with more than 1 word.<br /><br />Query: {query}<br /><br />{format_instructions}&quot;&quot;&quot;<br /><br />query_parser= PydanticOutputParser(pydantic_object=Query)<br />query_output_fixing_parser=OutputFixingParser(parser=query_parser,llm=llm)<br /><br />query_classifier_prompt=PromptTemplate(input_variables=[&quot;query&quot;],template=query_classifier_template,partial_variables={&quot;format_instructions&quot;:query_parser.get_format_instructions()})<br />query_classifier_runnable = query_classifier_prompt | llm  | query_output_fixing_parser</pre> <p>Now we’ll create separate runnables tailored for responding to each query type.</p> <pre>class Response(BaseModel):<br />    response : str = Field(description=&quot;Response provided as a response to the query&quot;)<br /><br />payment_issues_template=&quot;&quot;&quot;<br />You are a PayTM customer service representative. Introduce yourself first. Your job is to address the queries related to payment issues. Help the user out in 2 sentences. <br /><br />Query is {query}<br /><br />{format_instructions}&quot;&quot;&quot;<br /><br />delivery_issues_template=&quot;&quot;&quot;<br />You are a delhivery customer service representative. Introduce yourself first. Your job is to address the queries related to delivery issues. Help the user out in 2 sentences. <br /><br />Query is {query}<br /><br />{format_instructions}&quot;&quot;&quot;<br /><br />product_issues_template=&quot;&quot;&quot;<br />You are an Amazon customer service representative. Introduce yourself first. Your job is to address the queries related to product issues. Help the user out in 2 sentences. <br /><br />Query is {query}<br /><br />{format_instructions}&quot;&quot;&quot;<br /><br />response_parser=PydanticOutputParser(pydantic_object=Response)<br />response_output_fixing_parser=OutputFixingParser(parser=response_parser,llm=llm)<br /><br />payment_issues_prompt=PromptTemplate(input_variables=[&quot;query&quot;],template=payment_issues_template,partial_variables={&quot;format_instructions&quot;:response_parser.get_format_instructions()})<br />payment_issues_runnable = payment_issues_prompt | llm  | response_output_fixing_parser<br /><br />delivery_issues_prompt=PromptTemplate(input_variables=[&quot;query&quot;],template=delivery_issues_template,partial_variables={&quot;format_instructions&quot;:response_parser.get_format_instructions()})<br />delivery_issues_runnable = delivery_issues_prompt | llm | response_output_fixing_parser<br /><br />product_issues_prompt=PromptTemplate(input_variables=[&quot;query&quot;],template=product_issues_template,partial_variables={&quot;format_instructions&quot;:response_parser.get_format_instructions()})<br />product_issues_runnable = product_issues_prompt | llm | response_output_fixing_parser</pre> <p>Here’s the core logic to route queries to the correct runnables:</p> <pre>def logic(category):<br />    if &quot;payment&quot; in category[&#39;info&#39;].query_category.lower():<br />        return payment_issues_runnable<br />    elif &quot;delivery&quot; in category[&#39;info&#39;].query_category.lower():<br />        return delivery_issues_runnable<br />    else :<br />        return product_issues_runnable</pre> <p>category is the variable which will be returned by query_classifier_runnable which follows the Query class. The routing logic is simple, if ‘payment’ is present in the query_classifier_runnable response then we route the query to payment_issues_runnable . If ‘delivery’ is present in the query_classifier_runnable response then we route the query to delivery_issues_runnable else we route the query to product_issues_runnable . We declare that the response from the query_classifier_runnable object is stored in the info attribute of the category variable as declared below.</p> <pre>complete_classification_pipeline = {<br />    &quot;query&quot;: lambda x: x[&quot;query&quot;], <br />    &quot;info&quot;: query_classifier_runnable<br />} | RunnableLambda(logic) </pre> <p>The query variable is what will hold our input that we provide and pass it along to the respective prompts.</p> <p>Since complete_classification_pipeline is also a Runnable , it also inherits all the methods such as invoke() , batch() , stream() and so on. So to test the logic out, let’s use the invoke() method on some prompts.</p> <pre>result=complete_classification_pipeline.invoke({&quot;query&quot;:&quot;Your payment gateway crashed midway through payment and it withdrew money from my bank account without giving me the subscription. What can i do?&quot;})<br />result.response</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/972/1*bJnCzo6EGHvTK5TjFRbwXg.png"/><figcaption>Payment Handling</figcaption></figure> <pre>result=complete_classification_pipeline.invoke({&quot;query&quot;:&quot;The delivery driver hasn&#39;t arrived yet. What do I do?&quot;})<br />result.response</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/921/1*NzhFtuzi_ukpzfq_NJNabw.png"/><figcaption>Delivery Handling</figcaption></figure> <pre>result=complete_classification_pipeline.invoke({&quot;query&quot;:&quot;The headphones arrived faulty. What should I do?&quot;})<br />result.response</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/966/1*vNw0yelZrqP6FP-s7QysAA.png"/><figcaption>Product Handling</figcaption></figure> <p>As we can see, based on the query each of the queries went into their respective prompts through our defined routing and we successfully got proper feedback.</p> <p><strong>Summary:</strong></p> <p>The examples explored in this article represent just a fraction of what LangChain can achieve. From streamlined prototyping to complex multi-step systems, LangChain provides a flexible and robust framework for unlocking the full capabilities of large language models in your applications. There are many more things like Chatbots, RAG systems, Agents and so much more which can be made in LangChain.</p> <p>While LangChain’s documentation has room for improvement, it remains a valuable resource for your LLM development journey. You’ll also discover helpful insights from external sources, like this <a href="https://medium.com/artefact-engineering-and-data-science/unleashing-the-power-of-langchain-expression-language-lcel-from-proof-of-concept-to-production-8ad8eebdcb1d">Medium article</a> on LCEL. Now that you’ve grasped the basics, it’s time to unleash your creativity and start experimenting!</p> <h3>Follow For More!</h3> <p><em>I try to explore a lot of theoretical concepts in the ML space, with an emphasis on practical and intuitive applications.</em></p> <p><em>Thanks for reading this article! If you have any questions, I will be happy to answer them. Feel free to message me on my </em><a href="http://linkedin.com/in/suresh-raghu"><em>LinkedIn</em></a><em> or my </em><a href="http://sureshraghu0706@gmail.com/"><em>email</em></a><em> for other queries.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3521d725abf" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">From Images to Insights: 3-Tiered Data Extraction from images with OCR and Large-Language Models</title><link href="https://r-suresh07.github.io/blog/2023/from-images-to-insights-3-tiered-data-extraction-from-images-with-ocr-and-large-language-models/" rel="alternate" type="text/html" title="From Images to Insights: 3-Tiered Data Extraction from images with OCR and Large-Language Models"/><published>2023-11-17T10:20:59+00:00</published><updated>2023-11-17T10:20:59+00:00</updated><id>https://r-suresh07.github.io/blog/2023/from-images-to-insights-3-tiered-data-extraction-from-images-with-ocr-and-large-language-models</id><content type="html" xml:base="https://r-suresh07.github.io/blog/2023/from-images-to-insights-3-tiered-data-extraction-from-images-with-ocr-and-large-language-models/"><![CDATA[<p>Unpacking the Technology: How OCR and LLMs Are Redefining Data Capture from Images</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QVATTx6hbjur5bjMEmpI1Q.png"/><figcaption>Source: DALL-E-3</figcaption></figure> <p>Recently, I embarked on a personal project to track my spending by extracting information from images of grocery store receipts, inspired by apps like <a href="https://happay.com/expense-management-software/">Happay</a> or <a href="https://www.zoho.com/in/expense/receipt-tracking/">Zoho</a>. As I delved deeper into the project, I quickly realized the complexity of the problem I was facing. The core issue was the inconsistency in format and information placement across different receipts. This variability meant that a singular model couldn’t reliably extract all the necessary data.</p> <p>This personal challenge echoed a larger, industry-wide dilemma: the difficulty of processing information from documents with diverse layouts, not just limited to receipts but extending to various other forms like boarding passes or invoices. Traditional extraction methods, such as rule-based systems, proved insufficient due to their rigidity and inability to adapt to the wide range of document designs.</p> <p>Driven by my own experiences and the broader implications of this issue, my project aimed to develop a solution that was both flexible and efficient for extracting information from a variety of document formats. I focused on leveraging <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR (Optical Character Recognition)</a> and <a href="https://huggingface.co/docs/transformers/llm_tutorial">Large Language Models (LLMs)</a> like ChatGPT. This approach marked a shift from using OCR solely for digitizing text to a comprehensive process where OCR-generated unstructured text is transformed into structured data using the advanced contextual capabilities of LLMs.</p> <p>The innovation of this method lies in its adaptability, capable of handling numerous document types without extensive training or specific rules for each format. My goal was to demonstrate an effective strategy for data extraction that could be a reference point for similar challenges faced in various business contexts. This project was not just a technical pursuit but a journey that stemmed from a personal need to find a practical solution to an everyday problem.</p> <p>The GitHub link of the project can be found <a href="https://github.com/R-Suresh07/Information-Extraction-from-Images.git">here</a>.</p> <p>The Google Collab link of Methodology 3 can be found here:</p> <p><a href="https://colab.research.google.com/drive/14_WT-yUYJKfYrgLHKIE9nLc_EQIKOXRv?usp=sharing">Google Colaboratory</a></p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OCYubeFJ9jdLmmgt.PNG"/><figcaption>Source: <a href="https://www.semanticscholar.org/paper/Representation-Learning-for-Information-Extraction-Majumder-Potti/58877aa9aa2d09585a4ff4881b02cb1c7ff9bc28">[PDF] Representation Learning for Information Extraction from Form-like Documents | Semantic Scholar</a></figcaption></figure> <p><strong>Problem Statement</strong></p> <p>In the rapidly digitizing world, businesses and individuals frequently encounter the challenge of extracting precise information from a myriad of image-based documents like receipts, boarding passes, and invoices. Each of these document types comes with its unique layout and format, making the task of data extraction and processing both complex and time-consuming. Traditional methods often fall short due to their inflexibility and inability to handle the vast diversity in document designs.</p> <p>This project tackles the pressing need for a more adaptive and scalable approach to decipher and transform unstructured data from various image formats into structured, actionable information. It explores the intersection of Optical Character Recognition (OCR) and Large Language Models (LLMs) as a solution. The focus is on developing methods that not only accurately extract text from images but also intelligently interpret and organize this data, catering to the nuances of different document types.</p> <p>The aim is to bridge the gap between the simplicity of image-based data and the complexity of its extraction and structuring, providing efficient, cost-effective, and versatile solutions suitable for diverse applications in the digital era.</p> <p><strong>Value Propositions of this project</strong></p> <ol><li><strong>Versatility in Data Processing:</strong> Capable of handling a wide range of document formats and styles, making it suitable for various types of image-based data extraction tasks.</li><li><strong>Increased Efficiency:</strong> Streamlines the process of converting unstructured data from images into structured, usable formats, saving time and resources.</li><li><strong>Enhanced Accuracy:</strong> Combines the precision of advanced OCR technologies with the contextual understanding of LLMs, leading to more accurate data extraction and interpretation.</li><li><strong>Cost-Effectiveness:</strong> Offers solutions that range from open-source, free tools to more sophisticated, paid services, allowing for cost-effective implementation according to the user’s budget and needs.</li><li><strong>Data Security and Autonomy:</strong> Provides options that include in-house processing, minimizing data privacy concerns and ensuring greater control over the data extraction process.</li><li><strong>Scalability:</strong> Suitable for both small-scale individual use and large-scale enterprise applications, with potential for customization and scaling as per user requirements.</li><li><strong>Ease of Use:</strong> Despite the underlying complexity, the methodologies are designed to be accessible and implementable, even for those with limited technical expertise.</li><li><strong>Customizability:</strong> Offers the flexibility to mix and match OCR and LLM tools, and even fine-tune models for specific use cases, enhancing the relevance and effectiveness of the solutions.</li><li><strong>Innovation and Future-Proofing:</strong> Demonstrates the integration of cutting-edge AI technologies, positioning users at the forefront of technological advancements in data processing.</li></ol> <p><strong>Exploration</strong></p> <p>Having encountered these challenges in my personal project, I realized that the solution lies not just in a single technology, but in a combination of tools and approaches. This realization led me to explore and experiment with three distinct methodologies, each offering its unique advantages and drawbacks in the context of information extraction from images.</p> <p>After discussing the advantages and disadvantages of each methodology, it’s important to not only understand them theoretically but also practically. To bridge this gap, I will be providing detailed, step-by-step <a href="https://github.com/R-Suresh07/Information-Extraction-from-Images.git">code implementations</a> in python for each of these three methodologies in the subsequent sections of this article.</p> <p><strong>Methodology 1: GPT-4 Vision (GPT-V) by OpenAI</strong></p> <p>The first methodology I explored leverages the innovative capabilities of GPT-4 Vision (GPT-V), a state-of-the-art <a href="https://huggingface.co/tasks/visual-question-answering">Visual Question Answering (VQA)</a> model developed by OpenAI. GPT-V stands out for its ability to analyze images and provide detailed information based on user queries. This model allows users to upload an image and ask specific questions about its content, to which GPT-V responds with precise and relevant information.</p> <p>In the context of extracting data from images, GPT-V offers a highly intuitive and efficient solution. The process is remarkably simple: you upload an image to the model, and GPT-V processes this image to return the required information in a structured JSON format. This data can then be easily stored and retrieved for later use.</p> <p><strong>Advantages of Using GPT-V:</strong></p> <ul><li><strong>Simplicity and Efficiency:</strong> The process of uploading an image and receiving data in JSON format is straightforward, minimizing the complexity typically associated with data extraction.</li><li><strong>High Effectiveness:</strong> GPT-V employs the sophisticated capabilities of the GPT-4 model, ensuring accurate and reliable data extraction from images.</li></ul> <p><strong>Disadvantages of Using GPT-V:</strong></p> <ul><li><strong>Cost Considerations:</strong> Utilizing GPT-V can be costly, particularly when processing images with high resolution, which may be a significant factor for projects with budget constraints.</li><li><strong>Data Autonomy Concerns:</strong> As GPT-V is a third-party service, there are inherent issues regarding data privacy and control. This reliance on an external vendor may pose challenges for projects where data security and autonomy are paramount.</li><li><strong>API Latency and Rate Limits:</strong> There can be significant time delays in receiving information back from the API, and may have rate limits imposed on the account, affecting the scalability and responsiveness of the system.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/600/0*sPIBRjiw3FUSz16X.png"/><figcaption>Source: <a href="https://community.openai.com/u/wclayf">Summary — wclayf — OpenAI Developer Forum</a></figcaption></figure> <p><strong>Methodology 2: Azure Cognitive Services and GPT 3.5 Turbo by OpenAI</strong></p> <p>The second methodology I explored combines the use of Optical Character Recognition (OCR) and a Large-Language Model (LLM) to effectively process and structure data extracted from images. This method starts with an OCR model to capture all textual information from the image, followed by the utilization of an LLM to organize this unstructured data into a desired format.</p> <p>For this approach, I employed <a href="https://azure.microsoft.com/en-us/products/ai-services/ai-vision">Azure Cognitive Services (ACS)</a> as the OCR tool and <a href="https://platform.openai.com/docs/api-reference">GPT 3.5 Turbo</a> for data structuring. ACS excels in extracting all text present in an image, converting it from a visual format into a machine-readable, unstructured textual format. Following this, GPT 3.5 Turbo’s JSON formatter function is used to restructure this raw text. The integration of ACS’s powerful OCR capabilities with the advanced data structuring ability of GPT 3.5 Turbo results in highly accurate information extraction. The outcome is not just a simple transcription of the image’s text but a well-organized, structured JSON representation of the information originally contained in the image.</p> <p><strong>Advantages of Using ACS and GPT 3.5:</strong></p> <ul><li><strong>Accurate OCR:</strong> ACS provides high-quality text extraction.</li><li><strong>Advanced Data Structuring: </strong>GPT 3.5 Turbo effectively transforms unstructured text into structured JSON.</li><li><strong>Quality Results:</strong> Combines two powerful tools for better accuracy.</li></ul> <p><strong>Disadvantages of Using ACS and GPT 3.5:</strong></p> <ul><li><strong>Cost and Dependency:</strong> Although not as expensive as methodology 1, this approach still involves expenses for external services and dependency on third-party vendors.</li><li><strong>Data Privacy:</strong> Potential issues with data autonomy due to reliance on external processing.</li><li><strong>API Latency and Rate Limits:</strong> Similar to GPT-V, this method faces issues with time delays and potential rate limits on API usage, impacting real-time processing capabilities.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/1*b_29FYcpCJxEf4z8yhNKeg.png"/><figcaption>Flowchart for Methodology 2 (Source: Me)</figcaption></figure> <p><strong>Methodology 3: PaddleOCR and Zephyr-7b</strong></p> <p>The final approach I explored shares a similar template with the second methodology but shifts its focus towards data security and latency, while still maintaining a strong emphasis on result accuracy.</p> <p>For the OCR component, I opted for <a href="https://pypi.org/project/paddleocr/">PaddleOCR</a>. This toolkit, built on the PaddlePaddle framework, supports over 80 languages and offers a comprehensive suite of tools for text recognition and detection. Additionally, PaddleOCR is equipped with features for data annotation and synthesis, and it is optimized for various deployment environments, including servers, mobile, embedded, and IoT devices. Its lightweight nature makes it an ideal choice for a production environment, particularly where resource efficiency is a priority.</p> <p>Moving to the Large Language Model (LLM) aspect, I selected <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha">Zephyr-7B</a> developed by HuggingFace. Zephyr-7B is a fine-tuned version of the Mistral-7B model, which has demonstrated impressive performance, rivaling GPT-3.5 Turbo in many benchmarks, including MT-Bench and AlpacaEval. The efficiency of Zephyr-7B, despite being a 7-billion parameter model, means it is accessible for consumer-grade hardware, making it a practical choice for various applications.</p> <p>For those interested in learning more about Zephyr-7B and its development, I recommend the article <a href="https://gathnex.medium.com/zephyr-7b-beats-chatgpt-huggingface-open-challenge-to-openai-bba2fceecd9c">“Zephyr 7B Beta beats ChatGPT: HuggingFace’s Open challenge to OpenAI” by Gathnex</a> (published in October 2023) and exploring its <a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha">dedicated page on HuggingFace’s website</a>.</p> <p><strong>Advantages of using PaddleOCR and Zephyr-7B:</strong></p> <ul><li><strong>Cost-Efficient:</strong> No additional costs for external services.</li><li><strong>Data Security:</strong> Enhanced data privacy as processing is done within your ecosystem.</li><li><strong>Independence from APIs:</strong> The lack of reliance on external APIs eliminates concerns about rate limits and latency, with speed dependent only on the user’s hardware.</li></ul> <p><strong>Disadvantages</strong> <strong>of using PaddleOCR and Zephyr-7B</strong>:</p> <ul><li><strong>Infrastructure Requirements:</strong> Requires significant computational resources for scaling.</li><li><strong>OCR Accuracy Variances:</strong> Paddle-OCR’s accuracy might not suffice for certain use cases.</li><li><strong>Inference Speed:</strong> Processing speed is contingent on the available hardware, which could be a limitation for users with less powerful systems.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/404/1*BE2xtVSX4r0Xfo3KqqXD9g.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/224/1*UpUX_JxXrXxSj2c8ENDeBQ.png"/></figure> <p><strong>Transitioning to Implementations</strong></p> <p>Having explored the theoretical aspects and weighed the pros and cons of each methodology, it’s time to delve into the practical side. In the following sections, I will guide you through the step-by-step implementation of each methodology. These walkthroughs are designed to be clear and comprehensive, whether you’re a novice just starting out or an experienced developer looking to expand your toolkit.</p> <p><strong>Implementation of GPT-4 Vision (GPT-V)</strong></p> <p>Let’s first install the additional dependency required.</p> <pre>!pip install openai</pre> <p>Then let’s get all of our imports and API key sorted.</p> <pre>import base64<br />import requests<br />from openai import OpenAI<br /># OpenAI API Key<br />api_key = &quot;Insert-Your-OpenAi-API-Key-Here&quot;<br />client = OpenAI(api_key = api_key)</pre> <p>To start using GPT-V, you first need to create an account on OpenAI’s website and then generate <a href="https://platform.openai.com/api-keys">API keys</a>. It’s important to note that GPT-V access requires at least one completed payment on your account. Once these prerequisites are met, OpenAI will provide you with access to GPT-V.</p> <p>Now let’s write a helper function.</p> <pre>def encode_image(image_path):<br />    with open(image_path, &quot;rb&quot;) as image_file:<br />        return base64.b64encode(image_file.read()).decode(&#39;utf-8&#39;)<br /><br />headers = {<br />        &quot;Content-Type&quot;: &quot;application/json&quot;,<br />        &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;<br />}</pre> <p>Before making an API request, the local image must be converted into a format compatible with GPT-V. This is where the encode_image() function comes into play. It takes a locally stored image and converts it into its base64 encoded version, ready for the API request.</p> <p>Then we write the code which will help us send the API request.</p> <pre>def question_image(url,query):<br />    if url.startswith(&quot;http://&quot;)or url.startswith(&quot;https://&quot;):<br />        response = client.chat.completions.create(<br />            model=&quot;gpt-4-vision-preview&quot;,<br />            messages=[<br />            {<br />            &quot;role&quot;: &quot;user&quot;,<br />            &quot;content&quot;: [<br />                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;{query}&quot;},<br />                    {<br />                    &quot;type&quot;: &quot;image_url&quot;,<br />                    &quot;image_url&quot;: url,<br />                    },<br />                ],<br />            }<br />        ],<br />        max_tokens=1000,<br />            <br />        )<br />        return response.choices[0].message.content<br />    else:<br />        <br />        base64_image = encode_image(url)<br /><br />        payload = {<br />            &quot;model&quot;: &quot;gpt-4-vision-preview&quot;,<br />            &quot;messages&quot;: [<br />              {<br />                &quot;role&quot;: &quot;user&quot;,<br />                &quot;content&quot;: [<br />                  {<br />                    &quot;type&quot;: &quot;text&quot;,<br />                    &quot;text&quot;: f&quot;{query}?&quot;<br />                  },<br />                  {<br />                    &quot;type&quot;: &quot;image_url&quot;,<br />                    &quot;image_url&quot;: {<br />                      &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;<br />                    },<br />                  }<br />                ]<br />              }<br />            ],<br />            &quot;max_tokens&quot;: 1000<br />        }<br /><br />        response = requests.post(&quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, json=payload)<br /><br />        temp=response.json()<br />        return temp[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]</pre> <p>The question_image() function might initially seem complex, but it primarily consists of standard boilerplate code. Let’s break it down for a clearer understanding:</p> <p><strong>Function Parameters:</strong> The function accepts two key inputs:</p> <ul><li>url: This can either be a URL to an image online or a local file path.</li><li>query: The specific question or prompt you wish to ask about the image.</li></ul> <p><strong>Handling Online URLs: </strong>If the url is an online URL, the function executes the code within the &#39;if&#39; statement, preparing and sending the request to GPT-V.</p> <p><strong>Customization Options:</strong></p> <ul><li>system prompt: Set within the messages parameter, this allows you to direct the model to respond in a particular manner.</li><li>max_tokens: This can be adjusted as needed. I’ve set it to 1000 to balance detail in the response while managing token usage efficiently.</li><li>format_response: Altering this value in the API call can guide the model to respond strictly in well-structured JSON format. You can find out about format_response and other parameters in detail <a href="https://platform.openai.com/docs/guides/text-generation/json-mode">here</a>.</li></ul> <p><strong>Handling Local Files:</strong> If a local file is provided as url, the function enters the &#39;else&#39; block and executes a different type of API call. The same parameters (system prompt, max_tokens, format_response) can be adjusted here as well to tailor the API&#39;s response to your needs.</p> <p>Now, all that’s required is to pass an image along with a query to perform text extraction. For demonstration purposes, I’ll use the following image of a Singapore Airlines boarding pass. While simpler than a typical receipt, it still presents similar challenges in terms of information extraction. The task will be to ask GPT-V to extract key details such as the airline name, passenger name, flight number, departure city, destination city, and the date of departure.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/760/1*yqopnaKNVxe2jwC9n2QNLA.jpeg"/><figcaption>Image of a Singapore Airlines boarding pass</figcaption></figure> <pre>query=&quot;Extract the airline_name,passenger_name,flight_num,departure_city,destination_city and date_of_departure from this boarding pass in a JSON format&quot;<br />image_url=&quot;singapore.jpg&quot;<br />boarding_pass_json=question_image(image_url,query)<br />print(boarding_pass_json)</pre> <p>After passing the query and the image to the function, we receive the following response.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/316/1*kenfPNICrI4Qxy46VcPinQ.png"/><figcaption>JSON output of the boarding pass information</figcaption></figure> <p>Pretty neat if I do say so myself.</p> <p><strong>Implementation of Azure Cognitive Services and GPT 3.5 Turbo</strong></p> <p>Let’s install all pre-requiste libraries.</p> <pre>!pip install opencv-python<br />!pip install openai</pre> <p>Note: If you are having any trouble installing opencv, refer to the official docs for clarification here: <a href="https://pypi.org/project/opencv-python/">opencv-python · PyPI</a></p> <p>Now let’s get all the imports out of the way.</p> <pre>import os<br />from openai import OpenAI<br />import json<br />import cv2<br />import requests<br />import time<br />os.environ[&#39;OPENAI_API_KEY&#39;]=&#39;Insert-your-openai-api-key-here&#39;<br />client = OpenAI()</pre> <p>Setup your API key like explained above.</p> <p>First, lets define the function to perform OCR through Azure Cognitive Services.</p> <pre>subscription_key = &quot;Insert-Your-Azure-Cognitive-Services-Key-Here&quot;<br />def ocr(img,mime_type=&#39;image/jpeg&#39;):<br /><br />    &quot;&quot;&quot;<br /><br />    Ocr extraction from Image or Image at local path<br /><br />    Input: Image or Image local path<br /><br />    Output: Cognitive Services OCR output response json<br /><br />    &quot;&quot;&quot;<br />    #Change the url for your particular region. I live in India so i use the central India servers<br />    url = &quot;https://centralindia.api.cognitive.microsoft.com/vision/v3.2/read/analyze?language=en&quot;<br /><br />    payload={}<br /><br />    img_bytes=None<br /><br />    if type(img)==str:<br /><br />        files=[(&#39;image&#39;,(&#39;tmp.jpg&#39;,open(img,&quot;rb&quot;),mime_type))]<br /><br />    else:<br /><br />        img_bytes=img<br /><br />        files=[(&#39;image&#39;,(&#39;tmp.jpg&#39;,img_bytes,mime_type))]<br /><br />    headers = {&#39;Ocp-Apim-Subscription-Key&#39;: subscription_key}<br /><br />    response = requests.request(&quot;POST&quot;, url, headers=headers, data=payload, files=files, verify=False)<br /><br />    status_code = response.status_code<br /><br />    ocr_out=response.headers<br /><br />    if status_code== 202:<br /><br />        url=ocr_out[&#39;Operation-Location&#39;]<br /><br />        response = requests.request(&quot;GET&quot;, url, headers=headers, data=payload, verify=False)<br /><br />        ocr_out=response.json()<br /><br />        while ocr_out[&quot;status&quot;]==&quot;running&quot;:<br /><br />            time.sleep(1)<br /><br />            response = requests.request(&quot;GET&quot;, url, headers=headers, data=payload, verify=False)<br /><br />            ocr_out=response.json()<br /><br />    #print(ocr_out)<br />    return dict(ocr_out),img_bytes</pre> <p>Firstly, ensure you have your Azure Cognitive Services vision API key ready. Input this key into the subscription_key variable. Now, let&#39;s delve into the key aspects of the ocr() function.</p> <p>The ocr() function is designed to accept two parameters:</p> <ul><li>img: The image you wish to process. This can be either a local file path or an online image URL.</li><li>mime_type: This has a default value set, but can be adjusted if needed.</li></ul> <p>In most instances, you’ll provide an image URL to the ocr() function to obtain the output JSON from Azure Cognitive Services. While much of the function consists of standard boilerplate code, there is a crucial point to remember: the url variable needs to be tailored to the specific server you&#39;re using. In my case, I set it to the Central India server.</p> <p>Adjusting this variable ensures that your requests are directed to the correct Azure server, optimizing the performance and reliability of your OCR operations.</p> <p>Now let’s write the main function which incorporates the OCR output with GPT 3.5 Turbo.</p> <pre>def Image_to_JSON(image_path):<br />    # Perform OCR on the image and extract the text content<br />    result,img_bytes = ocr(image_path)<br />    data=result[&#39;analyzeResult&#39;][&#39;readResults&#39;]<br />    texts=[line[&#39;text&#39;] for item in data for line in item[&#39;lines&#39;]]<br /><br />    # Stores the OCR content extracted from the image in a string which can be fed into ChatGPT<br />    ocr_string = &quot; &quot;.join(texts)<br />    ocr_string=ocr_string.lower()<br /><br />    # Create a query for ChatGPT, including the OCR content and the required information<br />    ChatGPT_Query=f&quot;&quot;&quot;The following is a raw dump of an OCR output from a boarding pass of a flight. Extract the airline name,name of the passenger,flight number,departure city,destination city and date of departure from it. The Departure City and Destination City should be in their airport codes.<br />    The raw OCR data of boarding pass is {ocr_string}.<br />    &quot;&quot;&quot;<br />    format_instructions=&quot;&quot;&quot;The output should be in a JSON format where the keys should be &#39;airline_name&#39;,&#39;passenger_name&#39;,&#39;flight_num&#39;,&#39;departure_city&#39;,&#39;destination_city&#39; and &#39;date_of_departure&#39;.&quot;&quot;&quot;<br />    <br />    completion = client.chat.completions.create(<br />    model=&quot;gpt-3.5-turbo-1106&quot;,<br />    messages=[<br />    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{ChatGPT_Query} \n {format_instructions}&quot;}<br />    ],<br />    response_format={ &quot;type&quot;: &quot;json_object&quot; }<br />    )<br />    # Receive the JSON file<br />    data = completion.choices[0].message.content<br /><br />    # Return the JSON object<br />    return json.loads(data)</pre> <p>The Image_to_JSON() function is designed with a singular purpose: to take an image path (image_path), which can be either a local file path or an online URL, and return the data contained within the image in a JSON format.</p> <p>The function begins by sending the image to Azure using the ocr() function we defined earlier. The first five lines handle this process. Once Azure processes the image, the extracted text is retrieved and stored in an array. This array is then concatenated into a single string, referred to as ocr_string, which contains all the words found in the image, converted to lowercase. The decision to use lowercase is based on preliminary observations that the system processes text more effectively in this format, though further exploration might be needed.</p> <p>Next, the query for GPT 3.5 Turbo is prepared and stored in the ChatGPT_Query variable. For demonstration purposes, I&#39;m using the boarding pass scenario, so the query is tailored for extracting information specific to a boarding pass. However, this query can be easily modified for different information extraction tasks. The query also specifies the desired format for each piece of data.</p> <p>Another variable, format_instructions, is used to define the format in which we want the response, which in this case is JSON. This includes specifying all the keys and the types of values expected in the output.</p> <p>After sending this request to GPT 3.5 Turbo and receiving the response, we extract the JSON data as a dictionary. This dictionary is then returned to the user, who can utilize it as needed.</p> <p>It’s important to note that in the request, I have used the response_format parameter similar to what was mentioned in the implementation of Methodology 1. This approach can be adopted in the GPT-V call as well, if required.</p> <p>Now let’s test it out on the same boarding pass image I used for testing methodology 1.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/760/1*yqopnaKNVxe2jwC9n2QNLA.jpeg"/><figcaption>Image of a Singapore Airlines boarding pass</figcaption></figure> <pre>b=Image_to_JSON(r&quot;singapore.jpg&quot;)<br />import pprint #Just for demonstration purposes, can just use regular print<br />pprint.pprint(b)</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/302/1*vGJl68NRgF_FclH8haCK8w.png"/><figcaption>JSON output of Boarding pass through ACS and GPT 3.5</figcaption></figure> <p>As you can see, it works perfectly.</p> <p><strong>Implementation of PaddleOCR and Zephyr-7B</strong></p> <p>Let’s install all pre-requiste libraries for Zephyr-7B.</p> <pre>!pip install git+https://github.com/huggingface/transformers.git<br />!pip install accelerate</pre> <p>Pre-requisite libraries for PaddleOCR</p> <pre>!git clone https://github.com/PaddlePaddle/PaddleOCR.git<br />!python3 -m pip install paddlepaddle-gpu<br />!pip install &quot;paddleocr&gt;=2.0.1&quot;</pre> <p>Now let’s get all the imports out of the way.</p> <pre>import torch<br />from paddleocr import PaddleOCR<br />from transformers import pipeline</pre> <p>Now let’s write the OCR through PaddleOCR first.</p> <pre>ocr = PaddleOCR(use_angle_cls=True, lang=&#39;en&#39;,use_space_char=True,show_log=False,enable_mkldnn=True)<br /><br />img_path = &#39;singapore.jpg&#39;<br />result = ocr.ocr(img_path, cls=True)<br /><br />ocr_string = &quot;&quot;  <br />#Extract the text from the OCR result and concatenate it to ocr_string<br />for i in range(len(result[0])):<br />    ocr_string = ocr_string + result[0][i][1][0] + &quot; &quot;</pre> <p>In this part of the process, we initialize PaddleOCR and load it into the ocr variable, configuring it with several parameters. While many of these parameters, like use_angle_cls and use_space_char, are standard, I&#39;ve included an additional parameter named enable_mkldnn. This particular parameter enhances performance with minimal overhead, essentially providing a free performance boost. For a more detailed understanding of what each parameter does, I recommend consulting the <a href="https://github.com/PaddlePaddle/PaddleOCR">PaddleOCR documentation</a>.</p> <p>After setting up PaddleOCR, we proceed to pass the image path, stored in the img_path variable, to it. This image path is the same Singapore Airlines boarding pass that we used in the previous methodology. The OCR then processes the image, and we compile all the detected text into a single variable called ocr_string, similar to what was done in Methodology 2.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VJfHz3ZEa7p7iMXRnoK7gA.png"/><figcaption>Sample Output of ocr_string</figcaption></figure> <p>Now let’s move onto the LLM code.</p> <pre>pipe = pipeline(&quot;text-generation&quot;, model=&quot;HuggingFaceH4/zephyr-7b-alpha&quot;, torch_dtype=torch.bfloat16, device_map=&quot;auto&quot;)</pre> <p>We use the pipeline from the Transformers library to download and use zephyr-7b-alpha model from HuggingFace.</p> <pre># Each message can have 1 of 3 roles: &quot;system&quot; (to provide initial instructions), &quot;user&quot;, or &quot;assistant&quot;. For inference, make sure &quot;user&quot; is the role in the final message.<br />messages = [<br />    {<br />        &quot;role&quot;: &quot;system&quot;,<br />        &quot;content&quot;: &quot;You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.&quot;,<br />    },<br />    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: {ocr_string}&quot;},<br />]<br /># We use the tokenizer&#39;s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating<br />prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)</pre> <p>In this step, we craft a prompt tailored to our specific use case. For this demonstration, which focuses on boarding passes, the prompts are customized accordingly. However, it’s important to note that these prompts can be modified to suit a wide range of other use cases. Once the prompt is written, we utilize the pipeline class, which we defined earlier, to format our message into a structure that Zephyr-7B-alpha can interpret effectively. The resulting format of the prompt variable is then ready for processing.</p> <p>Here is how prompt looks after it has been formatted.</p> <pre>&lt;|system|&gt;<br />You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.&lt;/s&gt;<br />&lt;|user|&gt;<br />Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: THE PRIVATE ROOM SINGAPORE SUITES SUITES HENG KOK HONGMR HENG KOK HONG MR SQ*G Fron SINGAPORE TOSYD Date19JAN18 To SYDNEY Flight SQ 231 FronSIN Sulte sutte Boarding Group FlightSQ231 Boarding time 3F Terminal Gate 3F 3 12:15AM Date 19JAN18 19JAN18 00325 ETNo YOU ARE INVITED TO THE PRIVATE ROOM SKL GATE CLOSES 10 MINS BEFORE DEPARTURE A STAR ALLIANCEMEMBER 00325 ETNO&lt;/s&gt;<br />&lt;|assistant|&gt;</pre> <p>The next stage involves sending the formatted prompt to the Zephyr-7B model for information extraction.</p> <pre>outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)<br />print(outputs[0][&quot;generated_text&quot;])</pre> <p>Key variables in this process include max_new_tokens, which controls the number of new tokens the model can generate. A higher value in this variable allows for greater creativity and length in the message produced by the model. The variables temperature, top_k, and top_p are used to regulate the randomness of the model&#39;s responses.</p> <ul><li>temperature (float, optional, defaults to 1.0): This parameter controls the randomness of predictions by scaling the logits before applying softmax. <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation">A higher value results in more random completions, while a lower value makes the model more confident but also more conservative</a>.</li><li><a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation">top_k (int, optional, defaults to 50): This is the number of highest probability vocabulary tokens to keep for top-k-filtering</a>. <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation">During the sampling process, it restricts the pool of words to select from to the top k words</a>.</li><li>top_p (float, optional, defaults to 1.0): Also known as nucleus sampling, it is an alternative to Top-K sampling. <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation">Instead of sampling only from the most likely K words, in Top-P sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p</a>.</li></ul> <p>You can find more detailed information about these parameters and their usage in the <a href="https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation">Hugging Face documentation</a> and <a href="https://discuss.huggingface.co/t/order-of-execution-of-top-k-top-p-sampling-along-with-temperature/55569">this discussion on Hugging Face’s forum</a>. You can also find examples of their usage in this <a href="https://gist.github.com/MarcSkovMadsen/eae998fbcb299fae9e92ab0089e7eff8">GitHub Gist</a>.</p> <p>Once the model has processed the prompt, we then extract and print the JSON output. This output contains all the relevant information extracted from the image.</p> <p>This is how the entire end-to-end function will look.</p> <pre>def Image_to_JSON(image_path):<br />    # Perform OCR on the image and extract the text content<br />    result = ocr.ocr(image_path, cls=True)<br /><br />    ocr_string = &quot;&quot;  # Stores the OCR content extracted from the image in a string which can be fed into ChatGPT<br /><br />    # Extract the text from the OCR result and concatenate it to ocr_string<br />    for i in range(len(result[0])):<br />        ocr_string = ocr_string + result[0][i][1][0] + &quot; &quot;<br /><br />    messages = [<br />    {<br />        &quot;role&quot;: &quot;system&quot;,<br />        &quot;content&quot;: &quot;You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.&quot;,<br />    },<br />    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: {ocr_string}&quot;},<br />    ]<br />    # We use the tokenizer&#39;s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating<br />    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)<br />    outputs = pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)<br />    print(outputs[0][&quot;generated_text&quot;])<br />    return outputs[0][&quot;generated_text&quot;]</pre> <p>Here’s an overview of how the entire Image_to_JSON() function operates:</p> <ul><li>The function accepts an image URL as input.</li><li>It processes this input to return a JSON output generated by Zephyr-7B-alpha.</li></ul> <p>Let’s apply this function to the same Singapore Airlines boarding pass used earlier.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/760/1*yqopnaKNVxe2jwC9n2QNLA.jpeg"/><figcaption>Image of a Singapore Airlines boarding pass</figcaption></figure> <pre>&lt;|system|&gt;<br />You are a JSON converter which receives raw boarding pass OCR information as a string and returns a structured JSON output by organising the information in the string.&lt;/s&gt;<br />&lt;|user|&gt;<br />Extract the name of the passenger, name of the airline, Flight number, City of Departure, City of Arrival, Date of Departure from this OCR data: THE PRIVATE ROOM SINGAPORE SUITES SUITES HENG KOK HONGMR HENG KOK HONG MR SQ*G Fron SINGAPORE TOSYD Date19JAN18 To SYDNEY Flight SQ 231 FronSIN Sulte sutte Boarding Group FlightSQ231 Boarding time 3F Terminal Gate 3F 3 12:15AM Date 19JAN18 19JAN18 00325 ETNo YOU ARE INVITED TO THE PRIVATE ROOM SKL GATE CLOSES 10 MINS BEFORE DEPARTURE A STAR ALLIANCEMEMBER 00325 ETNO&lt;/s&gt;<br />&lt;|assistant|&gt;<br />Here&#39;s the JSON output:<br /><br />```json<br />{<br />  &quot;Passenger&quot;: {<br />    &quot;Name&quot;: &quot;HENG KOK HONG MR&quot;<br />  },<br />  &quot;Airlines&quot;: {<br />    &quot;Name&quot;: &quot;SQ&quot;<br />  },<br />  &quot;Flight&quot;: {<br />    &quot;Number&quot;: &quot;SQ 231&quot;,<br />    &quot;Departure&quot;: {<br />      &quot;City&quot;: &quot;SINGAPORE&quot;,<br />      &quot;Date&quot;: &quot;19JAN18&quot;,<br />      &quot;Time&quot;: &quot;12:15AM&quot;<br />    },<br />    &quot;Arrival&quot;: {<br />      &quot;City&quot;: &quot;SYDNEY&quot;<br />    },<br />    &quot;Boarding&quot;: {<br />      &quot;Group&quot;: &quot;3&quot;,<br />      &quot;Time&quot;: &quot;3F Terminal Gate 3F&quot;<br />    }<br />  },<br />  &quot;Other Information&quot;: [<br />    {<br />      &quot;Title&quot;: &quot;The Private Room Singapore Suites&quot;,<br />      &quot;Location&quot;: &quot;Heng Kok Hong MR&quot;<br />    },<br />    {<br />      &quot;Title&quot;: &quot;Star Alliance Member&quot;,<br />      &quot;ETNo&quot;: &quot;00325&quot;<br />    },<br />    {<br />      &quot;Title&quot;: &quot;Gate Closes&quot;,<br />      &quot;Time&quot;: &quot;10 Mins Before Departure&quot;,<br />      &quot;Location&quot;: &quot;SKL Gate&quot;<br />    }<br />  ]<br />}<br />```<br /><br />Note: The &quot;Other Information&quot; section has been added to contain any other details that may be relevant to the passenger&#39;s travel.</pre> <p>The results demonstrate that the function is quite effective, and with some fine-tuning of the prompts, it can be reliably used for extracting information from images.</p> <p><strong>Points to consider</strong></p> <p>As we wrap up the implementations, it’s essential to understand the underlying logic: essentially, any OCR can be paired with any Large Language Model (LLM) to parse and structure data. While I demonstrated these methodologies using popular tools like Azure Cognitive Services, PaddleOCR, GPT 3.5 Turbo, and Zephyr-7B, the concepts are not limited to these specific tools. They serve as examples to illustrate the process, but you’re encouraged to explore other OCR and LLM options that might better suit your specific needs or preferences.</p> <p>Furthermore, if you’re willing to invest additional effort into refining your data extraction process, consider fine-tuning your chosen LLM. Methods such as <a href="https://medium.com/towards-data-science/lora-intuitively-and-exhaustively-explained-e944a6bff46b">Low Rank Adaptation (LoRA)</a> can be highly effective. By training the LLM on a series of prompt-output pairs consisting of Raw OCR dumps and the output being how you desire the OCR to be formatted, you can significantly enhance its ability to parse OCR dumps and generate more consistent JSON outputs. This fine-tuning can tailor the model to be more adept at understanding and structuring the specific types of data you’re working with.</p> <p>Infact, you could use Methodology 2 of using Azure Cognitive Services and GPT 3.5 Turbo to generate these prompt-output pairs which you could use to fine-tune the LLM of your choice to eventually switch to Methodology 3.</p> <p>For those interested in exploring an approach similar to Methodology 1, you might want to investigate open-source Visual Question Answering models like <a href="https://llava-vl.github.io/">LLaVA 1.5</a> or <a href="https://huggingface.co/adept/fuyu-8b">Fuyu 8B</a>. These models offer functionalities akin to GPT-V and can be a great alternative, especially if you’re looking for open-source options. Implementing your solution with these models can provide you with a similar experience to GPT-V, potentially with the added benefits of customization and greater control over your data processing pipeline.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ASXh8eBF56J7ZenrgzIe3Q.png"/><figcaption>Example demonstrating the capabilities of LLaVa. Source: <a href="https://llava-vl.github.io/">LLaVA (llava-vl.github.io)</a></figcaption></figure> <p><strong>Conclusion</strong></p> <p>In conclusion, this project showcases the dynamic and evolving landscape of data extraction from images, demonstrating the effectiveness of combining OCR with Large Language Models. Whether through the simplicity and power of GPT-4 Vision, the precision of Azure Cognitive Services paired with GPT 3.5 Turbo, or the flexibility and security of PaddleOCR with Zephyr-7B, each methodology presents its unique advantages and potential applications.</p> <p>The key takeaway is the versatility and adaptability of these approaches. By understanding the principles behind each method, you can mix and match different OCR and LLM tools to suit your specific needs, even venturing into the realm of fine-tuning models for enhanced performance.</p> <p>This project is just a glimpse into that potential, encouraging further exploration and experimentation in the fascinating realm of visual data, text recognition, and language understanding.</p> <h3>Follow For More!</h3> <p><em>I try to implement a lot of theoretical concepts in the ML space, with an emphasis on practical and intuitive applications.</em></p> <p><em>Thanks for reading this article! If you have any questions, I will be happy to answer them. Feel free to message me on my </em><a href="http://linkedin.com/in/suresh-raghu"><em>LinkedIn</em></a><em> or my </em><a href="http://sureshraghu0706@gmail.com/"><em>email</em></a><em> for other queries.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0c07754813cc" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Revolutionizing Image Search with GPT-4 Vision</title><link href="https://r-suresh07.github.io/blog/2023/revolutionizing-image-search-with-gpt-4-vision/" rel="alternate" type="text/html" title="Revolutionizing Image Search with GPT-4 Vision"/><published>2023-11-07T15:37:19+00:00</published><updated>2023-11-07T15:37:19+00:00</updated><id>https://r-suresh07.github.io/blog/2023/revolutionizing-image-search-with-gpt-4-vision</id><content type="html" xml:base="https://r-suresh07.github.io/blog/2023/revolutionizing-image-search-with-gpt-4-vision/"><![CDATA[<h4>Unveiling Precision and Context in Visual Discovery with AI!</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*M8dVgc7AJuywlYqBmptbfA.png"/><figcaption>Source: Dall-E 3</figcaption></figure> <p>After OpenAI unveiled GPT-4 Vision on their website in September, I became captivated by the potential applications we could make with access to an API version of this technology. My anticipation was met with excitement today as <a href="https://openai.com/blog/new-models-and-developer-products-announced-at-devday">OpenAI announced the Vision API, accompanied by an array of new models and features, during their Developer Day presentation</a>.</p> <p>One of the compelling applications we can explore is conducting image searches without the need to compare text directly to an image. In this article, I will explain what GPT-4 Vision is, why it’s a game-changer in the realm of visual search, and how I utilized it to streamline the process of searching through images.</p> <p>The GitHub link for the project can be found <a href="https://github.com/R-Suresh07/Image-Search-using-GPT-4-Vision-API.git">here</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*bayqt55irDIs-3Qd.png"/><figcaption>Source: <a href="https://www.reddit.com/r/OpenAI/comments/158y87l/gpt4_vision_its_amazing_alpha_users/">GPT-4 vision it’s amazing (Alpha users) : r/OpenAI (reddit.com)</a></figcaption></figure> <p><strong>Problem Statement</strong></p> <p>The primary issue confronting current image search technology is its limited ability to understand and process visual content with the same nuance and depth as human perception. Traditional image search algorithms rely heavily on metadata and image recognition software that can misinterpret the context or content of an image, leading to inaccurate search results.</p> <p>The aim of this article is to demonstrate how GPT-4 Vision can be leveraged to overcome the limitations of conventional image search methods, thereby providing a more intuitive and user-friendly search experience, while also taking into account the cost and policy constraints associated with using such cutting-edge technology.</p> <p><strong>Value Proposition of the project</strong></p> <ol><li><strong>Enhanced Accuracy:</strong> By leveraging GPT-4’s advanced understanding of images and language, search results are more aligned with the users’ intent and context of the query.</li><li><strong>Contextual Understanding:</strong> GPT-4 Vision goes beyond keyword matching to comprehend the nuances and subtleties within images, thus providing results that account for context and content more deeply.</li><li><strong>Efficient Processing:</strong> By converting visual content into text descriptions, the system allows for faster and more efficient comparison of search queries with a database of images, reducing search time.</li><li><strong>Innovative Content Interaction:</strong> Users can interact with the search system in novel ways, such as asking complex questions about the contents of an image, which traditional image search engines cannot handle.</li><li><strong>Improved User Experience:</strong> By providing more accurate and relevant results, user satisfaction with the image search process is expected to increase.</li><li><strong>Reduced Infrastructure Demand:</strong> For businesses, utilizing GPT-4 Vision can mean less reliance on heavy computational infrastructure compared to running their own complex image processing and recognition systems.</li><li><strong>Flexibility and Adaptability: </strong>The system is adaptable to various types of image searches, ranging from simple object identification to complex scene descriptions, making it a versatile tool for multiple use cases.</li><li><strong>Future-proof Technology:</strong> By incorporating the state-of-the-art AI model, the system is poised to evolve with advancements in AI, ensuring long-term relevance and improvement.</li></ol> <p><strong>What is GPT-4 Vision?</strong></p> <p>GPT-4 Vision is a type of Visual Question Answering (VQA) model which allows the user to interact with GPT-4 not only through text but through images as well, allowing them to ask questions relating to the images.</p> <p>VQA is a research area in artificial intelligence that combines techniques from computer vision and natural language processing to answer questions about the contents of an image. GPT-4 Vision extends this concept by attaching a Large Language Model(GPT-4) to an image encoder which allows GPT-4 to understand visual information from images in addition to the textual information it already handles.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/761/1*s5jlquZuvu5iWoWhsenV2g.png"/><figcaption>Source: <a href="https://www.researchgate.net/publication/328128164_A_Novel_Approach_on_Visual_Question_Answering_by_Parameter_Prediction_using_Faster_Region_Based_Convolutional_Neural_Network">(PDF) A Novel Approach on Visual Question Answering by Parameter Prediction using Faster Region Based Convolutional Neural Network (researchgate.net)</a></figcaption></figure> <p><strong>Why is GPT-4 Vision so significant?</strong></p> <ul><li>GPT-4 Vision is a major advancement because it uses GPT-4, which is a cutting-edge language model, to understand and analyze images that users upload. This allows it to answer questions about these images more accurately than any other tool currently available.</li><li>It is versatile and can manage various tasks related to both text and images, such as describing, explaining, summarizing, translating, and generating content. It can also answer complex questions about the information it receives.</li><li>Moreover, GPT-4 Vision can generate creative content like poems, stories, code, essays, and songs based on the images and text it processes.</li><li>Safety and responsibility are at the forefront of GPT-4 Vision’s design. It has gone through rigorous testing to ensure it behaves safely, with built-in measures to prevent risks, especially in sensitive areas like identifying people, giving medical advice, or making assumptions without solid evidence.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Q96tZY19gckAUKEV"/><figcaption>Source:<a href="https://arxiv.org/abs/2303.08774?">[2303.08774] GPT-4 Technical Report (arxiv.org)</a> (Notably, the model is capable not just of recognizing the events in an image but alsoidentifying the humor in them.)</figcaption></figure> <p>The only comparable alternatives currently available are <a href="https://huggingface.co/adept/fuyu-8b">Fuyu-8B</a> and <a href="https://llava-vl.github.io/">LlaVa 1.5</a>, both of which are open-source. You can opt for either if you find that GPT-4 Vision does not meet your specific needs or if you need more control over your data.</p> <p><strong>Conventional Approach</strong></p> <p>In a typical image search algorithm, you might start by transforming the input text into a text embedding and converting the image into image embeddings. These are then compared using a model that operates within a shared embedding space, like <a href="https://openai.com/research/clip">CLIP</a>.</p> <p>However, there are several drawbacks to this approach, including:</p> <ul><li><strong>Limited Understanding: </strong>Traditional embedding-based methods may have limited capability in understanding the nuanced relationship between text and images. They might be good at matching direct correlations but can struggle with abstract concepts or interpreting the context of the images.</li><li><strong>Data Bias:</strong> The accuracy of embeddings often depends on the data they were trained on. If the dataset has biases or lacks diversity, the search results could be skewed, leading to less accurate or fair outcomes.</li><li><strong>Complex Queries:</strong> When users input complex or detailed search queries, these methods might not always provide accurate results because the algorithm may not capture all the nuances in both text and visual content.</li><li><strong>Computational Resources:</strong> Generating and comparing embeddings can be resource-intensive, requiring significant computational power, especially when dealing with large image databases.</li></ul> <p><strong>My Methodology</strong></p> <p>What if we could take a different approach by not directly using an image embedding model? Imagine if we pass our images to GPT-4 Vision and let it describe the images in words first. Then, we could use a superior text embedding model to process these descriptions. By comparing the text embeddings of the input and the image descriptions, we might enhance our results in several ways.</p> <p>Advantages of this new approach include:</p> <ul><li>Improved Accuracy: GPT-4 Vision can extract details from images more effectively than any open-source VQA model currently available, leading to more precise search results.</li><li>Reduced Search Time: It is generally faster and requires less computational effort to compare two sets of text embeddings than to compare text embeddings with image embeddings. This is because text embeddings typically have fewer dimensions; for instance, Word2Vec or GloVe models usually have dimensions in the hundreds (e.g., 300 for Word2Vec), while image embeddings from convolutional neural networks can have thousands or even tens of thousands of dimensions.</li><li>Space Efficiency: Text embeddings also occupy less disk space. Their lower dimensionality means they are more compact compared to the more dimensional image embeddings.</li></ul> <p><strong>Implementation</strong></p> <p>I am now going to implement an image search for an e-commerce store that specializes in selling luxury commodities such as liquor and perfumes. The dataset will have three columns: skuName, skuDescription, and imageUrls. The skuName will refer to the product name, the skuDescription will contain a 2-3 sentence description provided by the brand, and imageUrl will contain the URL of the product image.</p> <p>The dataset will be available in the GitHub repository <a href="https://github.com/R-Suresh07/Image-Search-using-GPT-4-Vision-API.git">here</a>. Note: The dataset is quite small (only 85 images) as I hit the rate limit for GPT-4 Vision, which is still in preview as of the time of writing this article. You can try scaling it up once it is fully available.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/846/1*TEjnGVdh4jBFqmqhl9Z66Q.png"/><figcaption>Sample of the Dataset</figcaption></figure> <p>Now, install all of these prerequisite libraries using pip or conda:</p> <pre>openai<br />numpy<br />pandas<br />matplotlib<br />pillow<br />FlagEmbedding </pre> <p>Let’s import all of these into our python file:</p> <pre>import pandas as pd<br />import numpy as np<br />import matplotlib.pyplot as plt<br />import requests<br />import base64<br />from openai import OpenAI<br />from PIL import Image<br />from FlagEmbedding import FlagModel</pre> <p>Before we proceed, ensure that you have access to GPT-4 Vision. You can do this by visiting OpenAI, creating a paid account, and generating an API key. Once you have your key ready, run the following code once:</p> <pre>import os<br />os.environ[&#39;OPENAI_API_KEY&#39;] = &#39;your-openai-key&#39;</pre> <p>Once you are done with that, let’s add some helper functions.</p> <pre>def load_image(url_or_path):<br />    if url_or_path.startswith(&quot;http://&quot;) or url_or_path.startswith(&quot;https://&quot;):<br />        return Image.open(requests.get(url_or_path, stream=True).raw)<br />    else:<br />        return Image.open(url_or_path)<br />def cosine_similarity(vec1, vec2):<br />    # Compute the dot product of vec1 and vec2<br />    dot_product = np.dot(vec1, vec2)<br />    <br />    # Compute the L2 norm of vec1 and vec2<br />    norm_vec1 = np.linalg.norm(vec1)<br />    norm_vec2 = np.linalg.norm(vec2)<br />    <br />    # Compute the cosine similarity<br />    similarity = dot_product / (norm_vec1 * norm_vec2)<br />    <br />    return similarity</pre> <p>The load_image() function is designed to load images from any URL, whether it&#39;s hosted online or located at a local file path. The cosine_similarity() function is employed to calculate the cosine similarity between two vectors. For an in-depth understanding of cosine similarity, please refer to <a href="https://medium.com/@sureshraghu0706/image-aesthetics-quantification-using-openai-clip-7bbb45e00147">my previous article</a>, where I provide a detailed explanation of this concept.</p> <p>Let’s now start the pre-processing stage.</p> <pre>def encode_image(image_path):<br />    with open(image_path, &quot;rb&quot;) as image_file:<br />        return base64.b64encode(image_file.read()).decode(&#39;utf-8&#39;)<br />headers = {<br />        &quot;Content-Type&quot;: &quot;application/json&quot;,<br />        &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;<br />}<br />client = OpenAI()<br />def question_image(url,query):<br />    if url.startswith(&quot;http://&quot;)or url.startswith(&quot;https://&quot;):<br />        response = client.chat.completions.create(<br />            model=&quot;gpt-4-vision-preview&quot;,<br />            messages=[<br />            {<br />            &quot;role&quot;: &quot;user&quot;,<br />            &quot;content&quot;: [<br />                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;{query}&quot;},<br />                    {<br />                    &quot;type&quot;: &quot;image_url&quot;,<br />                    &quot;image_url&quot;: url,<br />                        },<br />                    ],<br />                }<br />            ],<br />          max_tokens=1000,<br />        )<br />        return response.choices[0].message.content<br />    else:<br />        base64_image = encode_image(url)<br />        payload = {<br />            &quot;model&quot;: &quot;gpt-4-vision-preview&quot;,<br />            &quot;messages&quot;: [<br />              {<br />                &quot;role&quot;: &quot;user&quot;,<br />                &quot;content&quot;: [<br />                  {<br />                    &quot;type&quot;: &quot;text&quot;,<br />                    &quot;text&quot;: f&quot;{query}?&quot;<br />                  },<br />                  {<br />                    &quot;type&quot;: &quot;image_url&quot;,<br />                    &quot;image_url&quot;: {<br />                      &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;<br />                    },<br />                  }<br />                ]<br />              }<br />            ],<br />            &quot;max_tokens&quot;: 1000<br />        }<br /><br />        response = requests.post(&quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, json=payload)<br />        temp=response.json()<br />        return temp[&#39;choices&#39;][0][&#39;message&#39;][&#39;content&#39;]</pre> <p>The encode_image() function encodes any image into its base64 representation. This step is necessary for offline images that we wish to send to GPT-4 Vision.</p> <p>The question_image() function can take either an image URL or a local file path as input and applies distinct logic based on the nature of the input—whether it&#39;s an online resource or an offline file. This is because the payload sent to the OpenAI servers varies with the type of image source. The function requires two parameters: &#39;url,&#39; which specifies the location of the image, and &#39;query,&#39; which is the question or prompt related to the image and returns the answer provided by GPT-4 Vision.</p> <pre>data=pd.read_excel(r&quot;image_search_dataset.xlsx&quot;)<br />data[&#39;imageDescription&#39;]=None<br />query=&quot;Describe this image to me in detail&quot;<br />for i,row in data.iterrows():<br />    url=row[&#39;imageUrl&#39;]<br />    try:<br />        response = requests.get(url)<br />        if response.status_code == 200:<br />            desc=question_image(url,query)<br />            data.at[i,&#39;imageDescription&#39;]=desc<br />            data.to_excel(r&quot;image_search_dataset.xlsx&quot;,index=False)<br />    except requests.exceptions.RequestException:<br />        continue</pre> <p>In the given code, we start by reading our dataset into a variable named data. Next, we create a new column titled imageDescription to hold the descriptions that will be generated by GPT-4 Vision. We then loop through our dataset, and for each image, we check if the URL loads correctly (indicated by a status code of 200). If it does, we send the image URL to GPT-4 Vision with the request, “Describe this image to me in detail.” Upon receiving the description, we capture the response and save it to an Excel file. If a URL fails to load, we skip it and proceed to the next one.</p> <p>Now that we have compiled all of our image descriptions, the next step is to focus on the text embeddings. For this purpose, I referred to the <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB leaderboard on HuggingFace</a>, a benchmark that evaluates various text embedding models across multiple datasets and provides rankings. From there, I selected the top-performing open-source text embedding model, which is <a href="https://huggingface.co/BAAI/bge-large-en-v1.5">BAAI/bge-large-en-v1.5</a>. This model is particularly adept at managing large volumes of text data. You can choose another model according to your needs if need be. Just be mindful as different embedding models have different dimensionality and speed.</p> <pre>model = FlagModel(&#39;BAAI/bge-large-en-v1.5&#39;, <br />                  query_instruction_for_retrieval=&quot;Represent this sentence for searching relevant passages: &quot;,<br />                  use_fp16=True)</pre> <p>We import bge-large-en-v1.5 through FlagEmbeddings and set the query_instruction_for_retrieval according to the <a href="https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list">author’s recommendation for retrieval tasks</a>. You can also use either Sentence Transformers or HuggingFace pipelines to use this model.</p> <pre>data[&#39;embedding&#39;]=None<br />for i,row in data.iterrows():<br />    temp=row[&#39;skuName&#39;]+row[&#39;imageDescription&#39;]+row[&#39;skuDescription&#39;]<br />    data.at[i,&#39;embedding&#39;]=model.encode(temp)</pre> <p>Next, we loop through the dataframe once more. We construct a string variable named temp that concatenates the skuName, imageDescription, and skuDescription. This temp string is then fed into the bge-large model to generate embeddings. The resulting embeddings are stored back into the corresponding rows of the dataframe.</p> <p>We are finally done with all the pre-processing necessary for the project. Now let’s make the function which will combine all these elements.</p> <pre>def top_5_products(user_input):<br />    user_embedding=model.encode(user_input)<br />    data[&#39;scores&#39;]=None<br />    for i,row in data.iterrows():<br />        data.at[i,&#39;scores&#39;]=cosine_similarity(user_embedding,row[&#39;embedding&#39;])<br />    data[&#39;scores&#39;] = pd.to_numeric(data[&#39;scores&#39;], errors=&#39;coerce&#39;)<br />    top_5=data.nlargest(5,&#39;scores&#39;)<br />    fig, axs = plt.subplots(1, 5, figsize=(15, 3))  <br />    for i, row in enumerate(top_5.iterrows()):<br />        image = load_image(row[1][&#39;imageUrl&#39;])<br />        axs[i].imshow(np.array(image))<br />        axs[i].set_title(f&quot;{row[1][&#39;scores&#39;]}&quot;)<br />    plt.show()</pre> <p>The top_5_products() function initiates its process by taking a user&#39;s query and processing it through the bge-large model to produce a text embedding. We then utilize the cosine_similarity() function, previously defined, to compare this query embedding with all the description embeddings that were generated by GPT-4 Vision. These similarity scores are then added to the dataframe as a new column. Following this, we sort the dataframe to identify the top 5 products with the highest similarity scores and save this subset as top_5. In a live application, this top_5 dataset would be returned to the user. However, for demonstration purposes, I will present the images alongside their corresponding scores in a visual grid format using matplotlib.</p> <p>This was the entire end-to-end process behind designing an image search engine using GPT-4 Vision. Now let’s put this to the test.</p> <p><strong>Results</strong></p> <p>I will evaluate the program using four distinct types of queries: product type, product description, visual attributes, and sensory or unique characteristics. You will be able to review and assess the results firsthand.</p> <p>Query 1: Aberlour Double Cask 12 Year Old (Exact product name)</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1010/1*OhMt_qPgsPscmpEa9qwqtw.png"/><figcaption>Query 1 Results (Scores are arranged in descending order, with the highest score on the left and decreasing towards the right)</figcaption></figure> <p>Query 2: Scotch whiskey aged and mellowed (Search based on description)</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1004/1*aH9UEvteX3XBOgZtd48wKA.png"/><figcaption>Query 2 Results (Scores descending from left to right)</figcaption></figure> <p>Based on the brand’s description, we can see that Ballantine has been recommended the most by the algorithm.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KTmJBSWmo0mkUZBkrihKsw.png"/><figcaption>Product description of Ballantine 12 YO 100cl</figcaption></figure> <p>Query 3: Tall cylindrical bottle with a deep blue color (Search based on Image description)</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/998/1*KK6tViGNG37msxftbPNFhg.png"/><figcaption>Query 3 Results (Scores descending from left to right)</figcaption></figure> <p>We can see from the image description below why the algorithm chose Boss Bottled Night Eau De Toilette 200ml.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OVkRDY4zcE7pM6gxi4JrFg.png"/><figcaption>Image Description of Boss Bottled Night Eau De Toilette 200ml</figcaption></figure> <p>Query 4: Single malt with a sherry cask finish (Sensory attribute)</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1002/1*zOCNmi76xbxgR-2jAZSIHg.png"/><figcaption>Query 4 results(Scores descending from left to right)</figcaption></figure> <p>We can see that the algorithm is able to suggest Auchentoshan HeartwoodÂ 100cl because of it’s image description provided by GPT-4 Vision.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BlbFxDRSsuts7bydyM3j_g.png"/><figcaption>Image Description of Auchentoshan HeartwoodÂ 100cl</figcaption></figure> <p><strong>Scope for Improvements</strong></p> <p>While GPT-4 Vision has brought remarkable improvements to image search technology, there are specific areas where further development could enhance its performance and usability:</p> <ul><li><strong>Enhanced Text Embedding Models:</strong> Future updates could integrate text embedding models with larger sequence lengths than those of bge-large. This improvement would allow for a richer retention of product information, enhancing the precision of search results.</li><li><strong>Advanced Retrieval Techniques:</strong> As the project scales, it would be beneficial to transition from using a dataframe and cosine similarity to employing vector databases with clustering algorithms. This would enable faster and more efficient retrieval of search results, especially in systems with a vast image database.</li><li><strong>Cost Optimization Strategies: </strong>Considering the cost of $0.01105 per high-detail image of size 1366px X 768px, exploring cost-saving measures is crucial. Utilizing lower detail settings could reduce expenses significantly, to around $0.85 per 1000 images. However, this cost-cutting could impact the quality of results, necessitating a balance between detail and expense.</li><li><strong>Content Policy Adaptations:</strong> GPT-4 Vision’s strict content policy may restrict its ability to describe certain types of images, such as those involving fashion models. Finding alternative solutions that comply with content guidelines yet maintain descriptive accuracy will be essential for broadening the application scope of the technology.</li><li><strong>Open-Source Alternatives:</strong> In scenarios where GPT-4 Vision’s costs or content policies pose constraints, open-source alternatives like Fuyu-8B or LlaVa 1.5 could be considered, despite the potential increase in hardware requirements and possible compromise on performance.</li></ul> <p><strong>Conclusion</strong></p> <p>In summary, the integration of GPT-4 Vision into image search algorithms has the potential to revolutionize the way we interact with and retrieve visual information. This article walked you through the concept of GPT-4 Vision, its underlying mechanism, and a practical application that enhances image search functionality.</p> <p>By employing GPT-4 Vision, we’ve seen that not only can search accuracy be significantly improved by leveraging more descriptive image interpretations, but search efficiency can also be enhanced due to the reduced computational load of comparing text embeddings over traditional image-text comparisons.</p> <p>Moreover, the practical application and testing of this technology across various query types have demonstrated its robustness and versatility, catering to a wide spectrum of informational and sensory attributes.</p> <p>As we stand on the brink of this new era, the implications of such advancements reach far beyond mere convenience. They pave the way for more accessible digital environments, where the visual web can be navigated with the same ease as text-based information, opening up new possibilities for users with visual impairments or those seeking more intuitive search experiences.</p> <p>As we continue to fine-tune and integrate these models into our systems, one thing remains clear: the way we search, discover, and interact with images is set to evolve in extraordinary ways, making the act of finding not just an outcome, but an experience in itself.</p> <h3>Follow For More!</h3> <p><em>I try to implement a lot of theoretical concepts in the ML space, with an emphasis on practical and intuitive applications.</em></p> <p><em>Thanks for reading this article! If you have any questions, I will be happy to answer them. Feel free to message me on my </em><a href="http://linkedin.com/in/suresh-raghu"><em>LinkedIn</em></a><em> or my </em><a href="http://sureshraghu0706@gmail.com/"><em>email</em></a><em> for other queries.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e1fc36fca7e8" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Can we improve conventional reverse image search methods using both visual and textual analyses?</title><link href="https://r-suresh07.github.io/blog/2023/can-we-improve-conventional-reverse-image-search-methods-using-both-visual-and-textual-analyses/" rel="alternate" type="text/html" title="Can we improve conventional reverse image search methods using both visual and textual analyses?"/><published>2023-10-28T12:31:44+00:00</published><updated>2023-10-28T12:31:44+00:00</updated><id>https://r-suresh07.github.io/blog/2023/can-we-improve-conventional-reverse-image-search-methods-using-both-visual-and-textual-analyses</id><content type="html" xml:base="https://r-suresh07.github.io/blog/2023/can-we-improve-conventional-reverse-image-search-methods-using-both-visual-and-textual-analyses/"><![CDATA[<p>By leveraging the capabilities of OpenAI’s CLIP to analyze both imagery and associated text, we can dramatically enhance search quality, providing a comprehensive, dual-faceted approach.</p> <figure><img alt="Illustration of a split digital panel. On the left, a magnifying glass scans a colorful artwork, converting it into binary codes. On the right, the panel displays a range of similar artworks based on the scan. Floating between the panels, the OpenAI logo shines brightly, emitting rays of light that merge the visual and textual data, emphasizing the dual approach of the technology." src="https://cdn-images-1.medium.com/max/1024/1*aXNzXp-XjKkJd_3hfWq1GA.png"/><figcaption>Source: DALL-E-3</figcaption></figure> <p>In the digital age, even a single image can encapsulate vast amounts of information, narratives, and subtle details. But imagine if our machines could decipher images not merely by their visual elements, but also by the text they ‘read’. This principle is at the heart of OpenAI’s CLIP, a tool poised to bridge the gap between Computer Vision and Natural Language Processing. In this article, I’ll guide you step by step, demonstrating how we can harness the full potential of CLIP for advanced image searching.</p> <p>The GitHub link for this project can be found <a href="https://github.com/R-Suresh07/Reverse-Image-Search-CLIP.git">here</a>.</p> <p><strong>Problem Statement</strong></p> <p>Our objective is to illustrate that the integration of textual features, extracted by CLIP, along with image features derived from the same source, can enhance the overall accuracy. This combined approach may yield superior results compared to traditional reverse image search systems that rely solely on image similarity.</p> <p><strong>Value Proposition of the project</strong></p> <ul><li><strong>Superior Image Search</strong>: This project utilizes OpenAI’s CLIP to analyze both images and their associated text, significantly improving the quality of search results by providing a more comprehensive approach.</li><li><strong>Understanding Context and Text</strong>: Unlike traditional reverse image search systems that rely solely on image similarity, this project can find matches based on patterns, context, and the text within the image, addressing the limitations of traditional systems.</li><li><strong>Comprehensive Image Analysis</strong>: By integrating text similarity capabilities of CLIP with image similarity methods, the project can deliver more accurate and relevant search results. It can also handle cases where the exact image or brand is not in the dataset but a close match is available, offering a more holistic understanding of images.</li></ul> <p><strong>What is Reverse Image Search?</strong></p> <p>Reverse image search is like asking the search engine, “Where else have you seen this picture?” Instead of typing in words to find images, you use an image to find similar or related images and information about them.</p> <p>Traditionally, reverse image search works by comparing the uploaded image’s patterns, colors, and certain defining features against a vast database of images. Popular platforms like Google Images and TinEye have made this functionality widely accessible.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/320/1*VARLpznQbggywfmKGP7x6w.png"/><figcaption>Google reverse image search</figcaption></figure> <p>But these conventional methods have their downsides:</p> <ul><li><em>Limited Context Understanding:</em> These systems focus on pattern and color matching, often lacking context or deeper understanding of the image’s content.</li><li><em>Misidentification:</em> Similar patterns or colors in different images might lead to irrelevant search results.</li><li><em>Inability to Extract Textual Information: </em>Most traditional systems struggle with understanding or matching based on textual content within images.</li></ul> <p>Now this is where CLIP steps in.</p> <p>CLIP, developed by OpenAI, can understand both visual and textual content in images. This means it can find matches based on patterns and context, as well as the text within the image. It addresses the downsides by offering a more holistic image understanding, thereby potentially providing more accurate and relevant search results.</p> <p>For more in-depth information on CLIP, you can check out my previous article where I delved deeper into CLIP.</p> <p><a href="https://medium.com/@sureshraghu0706/image-aesthetics-quantification-using-openai-clip-7bbb45e00147">Image aesthetics quantification using OpenAI CLIP</a></p> <p><strong>Methodology:</strong></p> <p>For this project, I’ve curated a dataset consisting of 30 liquor images, with their respective names used as the filenames. You’re welcome to modify this to fit your specific requirements. Here’s a glimpse of the dataset:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/422/1*c96m7l8UwHc1OZx_IFlKCA.png"/><figcaption>Dataset Sample</figcaption></figure> <p><a href="http://dummy">Download the dataset here.</a></p> <p>There are two parts to the search: image similarity and text similarity. One might think that a simple image similarity with a good threshold would be suitable for this. However, let me provide you with a case where it doesn’t perform to the best of its capabilities.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/119/1*XZo9zSTEk5rAmnI7Bigk4w.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/361/1*8w_90sVLSxCKHFzA33h22g.png"/><figcaption>Absolut Citron( Left from Dataset, Right from Query)</figcaption></figure> <p>Let’s imagine a user is searching for the image shown on the right. Meanwhile, the image on the left represents our database’s entry. The two images differ, perhaps because the one on the right is from a recent bottle redesign or represents a special edition. Consequently, it doesn’t match the design stored in our database.</p> <p>When we apply only the Image Similarity technique to the image on the right, the results are as follows:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/410/1*k2gDlkcXV16eIPrvCP-cHg.png"/><figcaption>Results from regular image similarity</figcaption></figure> <p>Interestingly, the most relevant result displays a completely different brand. This discrepancy might arise because the shape and color of the bottle on the right bear a closer resemblance to Bacardi Ocho rather than Absolut Citron. Furthermore, observing the remaining results, one would logically expect all the Absolut bottles to rank higher before showcasing other brands. However, that isn’t the case here.</p> <p>To rectify this, we can enhance our search by extracting text features from the bottle and comparing them with the labels of all the bottles in our database. When employing the search logic I’ve developed (which I’ll explain shortly), the results are significantly improved:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/368/1*wTOaH1T855N8kQWe6ZBB2w.png"/><figcaption>Results from combining image and text features</figcaption></figure> <p>As evident, the search outcomes are now much more relevant, with other pertinent SKUs making their appearance.</p> <p><strong>Implementation:</strong></p> <p>We will be using Python 3.10 to implement the search. We start by first installing all of the following dependencies. We will explore the usage of each one shortly.</p> <pre>!pip install numpy==1.24.4<br />!pip install pandas==2.0.3<br />!pip install Pillow==10.1.0<br />!pip install Requests==2.31.0<br />!pip install streamlit==1.27.2<br />!pip install torch<br />!pip install transformers</pre> <p>Let’s also get all the imports and loading the model out of the way:</p> <pre>import torch<br />from PIL import Image<br />from transformers import AutoProcessor, CLIPModel<br />import torch.nn as nn<br />import requests<br />from io import BytesIO<br />import os<br />import numpy as np<br />import matplotlib.pyplot as plt<br />import pickle<br />device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &quot;cpu&quot;)<br />processor = AutoProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br />model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;).to(device)</pre> <p>Here we use the transformers library to import the base model of CLIP. We are using the base model just for demonstration purposes, but you can use larger models for better accuracy but just beware that larger models take more system requirements and cause more latency for giving results. You could also use the <a href="https://huggingface.co/sentence-transformers/clip-ViT-B-32">sentence-transformers</a> library to use CLIP in your program.</p> <p>As mentioned earlier, the logic is split into two parts:</p> <ul><li>Image Similarity</li><li>Text Similarity</li></ul> <p><strong>Image Similarity</strong></p> <p>First, let&#39;s write a couple of helper functions.</p> <pre>def load_image(image_path):<br />    if image_path.startswith(&quot;http://&quot;) or image_path.startswith(&quot;https://&quot;):<br />        return Image.open(requests.get(image_path, stream=True).raw)<br />    else:<br />        return Image.open(image_path)<br />def cosine_similarity(vec1, vec2):<br /># Compute the dot product of vec1 and vec2<br />    dot_product = np.dot(vec1, vec2)<br />    <br />    # Compute the L2 norm of vec1 and vec2<br />    norm_vec1 = np.linalg.norm(vec1)<br />    norm_vec2 = np.linalg.norm(vec2)<br />    <br />    # Compute the cosine similarity<br />    similarity = dot_product / (norm_vec1 * norm_vec2)<br />    <br />    return similarity</pre> <p>The load_image() function takes a URL, whether it be a local or an online URL, and converts it into a PIL Image object that we can manipulate in Python. The cosine_similarity() function is used to compute the similarity between two NumPy vectors. If you want to learn more about Cosine Similarity, follow the link below the image to an insightful article by AITechTrend, where they explain it in greater detail.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/643/1*0JblW1Xw81hU3w5CXdOM5w.png"/><figcaption><a href="https://aitechtrend.com/how-cosine-similarity-can-improve-your-machine-learning-models/">Cosine Similarity</a></figcaption></figure> <pre>images = []<br />img_skus=[]<br />for root, dirs, files in os.walk(r&quot;/Liquor_images&quot;):<br />    for file in files:<br />        if file.endswith(&#39;png&#39;):<br />            images.append(root  + &#39;/&#39;+ file)<br />img_skus=images.copy()<br />for i in range(len(img_skus)):<br />    img_skus[i]=os.path.basename(img_skus[i])[:-4]</pre> <p>We begin by loading all the image file paths from our dataset into the images array, and their corresponding names into the img_skus array. The dataset is stored in the same directory, under a folder named Liquor_images.</p> <pre>def extract_features_clip(image):<br />    with torch.no_grad():<br />        inputs1 = processor(images=image, return_tensors=&quot;pt&quot;).to(device)<br />        image_features = model.get_image_features(**inputs1)<br />    return image_features</pre> <p>Here, we’ve written a function called extract_features_clip(). This function takes an Image object as input and returns all the image features extracted from it using CLIP.</p> <pre>total_image_features=[]<br />for image_path in images:<br />    img = load_image(image_path)<br />    clip_feature = extract_features_clip(img)<br />    total_image_features.append(clip_feature)</pre> <p>Next, we iterate over the images array, converting the images at each filepath into image vectors. These vectors are stored in the total_image_features array. This is done by passing the images through the extract_features_clip() function that we wrote earlier.</p> <pre>filename=&#39;images.pkl&#39;<br />with open(filename,&#39;wb&#39;) as f:<br />    pickle.dump(total_image_features,f)</pre> <p>We then store all our embeddings in a pickle file. This allows us to save them on a storage device, from where we can retrieve them later. This eliminates the need to recompute the embeddings every time we want to perform a search. This completes all the preprocessing needed for the images.</p> <pre>with open(filename,&#39;rb&#39;) as f:<br />    features=pickle.load(f)<br /><br />def image_similarity(url):<br />    source=url<br />    image = load_image(source)<br />    #image=url<br />    image_features=extract_features_clip(image)<br />    similarity_scores = [cosine_similarity(image_features, i[0]) for i in features]<br />    merged_dict=dict(zip(img_skus,similarity_scores))<br />    sorted_dict = dict(sorted(merged_dict.items(), key=lambda item: item[1],reverse=True))<br />    return sorted_dict</pre> <p>Finally, we arrive at the part where we compute the image similarity. We first retrieve the image embeddings we previously created from the pickle file and store them in an array. When the image_similarity() function receives a URL, it converts the URL into an Image object using load_image(). We then compute the image embeddings by passing it through extract_features_clip(). Next, we compute the cosine similarity between the query image and all the vectors in the database using the cosine_similarity() function. We create a dictionary where the keys are the names of the most similar bottles and the values are their cosine similarity scores with the input image. Finally, we return this dictionary to the user which contains the search results.</p> <p>Now let’s move onto the text similarity score.</p> <p><strong>Text Similarity</strong></p> <pre>product_names = [<br />    &quot;Royal Salute 21 YO 70cl&quot;,<br />    &quot;Aberfeldy 12 YO Sp&quot;,<br />    &quot;Aberlour A Bunadh 70cl&quot;,<br />    &quot;Absolut 100 100cl&quot;,<br />    &quot;Absolut Citron 100cl&quot;,<br />    &quot;Absolut Elyx 100cl&quot;,<br />    &quot;Absolut Lime 100cl&quot;,<br />    &quot;Absolut Mandrin 100cl&quot;,<br />    &quot;Absolut Mango 100cl&quot;,<br />    &quot;Absolut Pears 100cl&quot;,<br />    &quot;Absolut Vodka Blue 100cl&quot;,<br />    &quot;Absolut Vodka Grapefruit 100cl&quot;,<br />    &quot;Bacardi Carta Blanca 100cl&quot;,<br />    &quot;Bacardi Ocho&quot;,<br />    &quot;Baileys Original 100 CL&quot;,<br />    &quot;Baileys Salted Caramel 100cl&quot;,<br />    &quot;Ballantine&#39;s 30 YO 70cl&quot;,<br />    &quot;Beluga Celebration 100 CL&quot;,<br />    &quot;Beluga Gold Line 1 L&quot;,<br />    &quot;Ballantine&#39;s Finest Twin Pack Cary 2x100cl&quot;<br />]<br /><br />class_names=product_names<br /># Compute text features for class names<br />text_inputs = processor(text=class_names, return_tensors=&quot;pt&quot;, padding=True).to(device)<br />with torch.no_grad():<br />    text_features = model.get_text_features(**text_inputs)</pre> <p>We create an array called product_names that contains the names of all the images in our dataset. Ideally, these names should closely match what is written on the labels of the liquor bottles. This array will serve as the class names to which CLIP will classify images. We then pass product_names to CLIP to extract all text features and store them in an array called text_features. You can also store this array as a pickle file for later retrieval, similar to how we stored image embeddings in a pickle file.</p> <pre>def predict(img_url):<br />    image1 = load_image(img_url)<br />    #image1=img_url<br />    image_features1=extract_features_clip(image1)<br />    cos_sim = nn.CosineSimilarity(dim=-1)<br />    similarity_scores = cos_sim(image_features1, text_features)<br />    similarity_scores=similarity_scores.tolist()<br />    merged_dict=dict(zip(product_names,similarity_scores))<br />    sorted_dict = dict(sorted(merged_dict.items(), key=lambda item: item[1],reverse=True))<br />    return sorted_dict</pre> <p>We then write a function called predict(), which takes in an image URL and converts it into an Image object using load_image(). We extract the image features from the query image using extract_features_clip(). We then compute the cosine similarity of the image vector to all textual vectors. Normally, this isn’t possible as image vectors and text vectors occupy different embedding spaces, but CLIP’s vectors are different. The entire design philosophy of CLIP is that both images and text share the same embedding space. This allows us to compare text vectors with image vectors. Returning to the program, we compute a dictionary similar to how we did in the image_similarity() function and return it to the user.</p> <p>Now that we have one function to extract textual features and another to extract image features, all we need to do is combine them and apply some sort of logic to rank them.</p> <pre>def image_search(url):<br />    text_similarity=predict(url)<br />    img_similarity=image_similarity(url)<br />    result={}<br />    for k,v in text_similarity.items():<br />        result[k]=0.5*text_similarity[k]*3+0.5*img_similarity[k]<br />    top_5_keys = sorted(result, key=result.get, reverse=True)[:5]<br />    return top_5_keys</pre> <p>We finally define a function called image_search(), which takes an image URL as input. This URL is passed to both the predict() and image_similarity() functions, and the score dictionaries are stored as text_similarity and img_similarity, respectively. We then create a final dictionary called result, where we compute the results and store the final ranking. Finally, we retrieve the top five keys with the highest values and return them to the user. If you observe the ranking logic, you’ll notice that I’ve assigned a 75% weightage to the text similarity score and a 25% weightage to the image similarity score. This is because when comparing text to an image, you’ll find that the score is significantly lower than when comparing an image to another image. To compensate for this, I’ve arbitrarily multiplied the text similarity score by 3. You can fine-tune and adjust this value if you believe there’s a better one</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/418/1*jbpE8f7O7SptIyu08D_80w.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/445/1*z58SR5MUNrM3DP_IxnQ1tA.png"/><figcaption>Text Scores(Left) and Image Scores(Right)</figcaption></figure> <p><strong>Optional: Streamlit Demo</strong></p> <p>If you wish to demonstrate your search to someone, you can use the following code as a template. Just ensure that you adjust the predict() and image_similarity() functions to accept an Image object as input, rather than a URL.</p> <pre>import streamlit as st<br />st.header(&#39;Image Search App&#39;)<br /><br />uploaded_file = st.file_uploader(&quot;Choose an image...&quot;, type=[&#39;png&#39;,&#39;jpg&#39;,&#39;jpeg&#39;])<br />picture_width = st.sidebar.slider(&#39;Picture Width&#39;, min_value=100, max_value=500)<br />if uploaded_file is not None:<br />    image = Image.open(uploaded_file)<br />    st.subheader(&#39;Input&#39;, divider=&#39;rainbow&#39;)<br />    st.image(image, caption=&#39;Uploaded Image&#39;, width=picture_width)<br /><br />        # Call your function with the uploaded image<br />    results = image_search(image)<br />    <br />    st.subheader(&#39;Results&#39;, divider=&#39;rainbow&#39;)<br />        # Display the results<br />    for product in results:<br />        product_image_path = os.path.join(r&#39;Liquor_images&#39;, f&#39;{product}.png&#39;)<br />        product_image = Image.open(product_image_path)<br />        st.image(product_image, caption=product, width=picture_width)</pre> <p><strong>Demonstration</strong></p> <p>Let’s put this system to the test. I will demonstrate the search on 3 images.</p> <p>Image 1</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ShXl-LJpC5NDELv0xFMCMg.png"/><figcaption>Image of Bacardi Carta Blanca</figcaption></figure> <p>Results for Image 1</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/938/1*Wq0q9KvYR4kGRhw20XHwmw.png"/><figcaption>Results page 1</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/936/1*naTPjIEWnDr2HtLUzP5I3Q.png"/><figcaption>Results page 2</figcaption></figure> <p>Image 2</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/172/1*hPloDAS_12vPkENSm0gkhA.jpeg"/><figcaption>Absolut Raspberry (Not in Dataset)</figcaption></figure> <p>Results for Image 2</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/924/1*vJAgYclbdIMgbHgH6cs4_Q.png"/><figcaption>Results page 1</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/877/1*AQjCWfefTN8y9qfUztzyOQ.png"/><figcaption>Results page 2</figcaption></figure> <p>We can see that even when the image isn’t in the dataset, but the brand is, the program is able to identify the most relevant brand. Moreover, the top result is the closest color to raspberry, which is grapefruit.</p> <p>Image 3</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*R4lS1r8U1kJbiyO-wv8uvw.jpeg"/><figcaption>Jack Daniels (Neither the brand nor the image is in the dataset)</figcaption></figure> <p>Results for Image 3</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/939/1*AQgEbu7ArEupk84whvn6Tw.png"/><figcaption>Results page 1</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/942/1*HH7_IUolZTlgOKBlHebTDw.png"/><figcaption>Results page 2</figcaption></figure> <p>We can see that when the image is searched, if the exact SKU and brand aren’t in the dataset, it returns the image that looks closest to the provided image. Hence, the top three results are all brown-colored bottles, which resemble the provided Jack Daniel’s bottle the most. We can observe that the scores of these images are lower compared to Image 1 and Image 2, where we found exact matches either in the SKU or the brand.</p> <p><strong>Conclusion:</strong></p> <p>So that&#39;s about it folks, we have successfully enhanced the basic image similarity-based reverse image search methods by integrating the text similarity capabilities of CLIP which gives more intelligent results. If you found a flaw in my logic, feel free to message me.</p> <h3><em>Follow For More!</em></h3> <p><em>I try to implement a lot of theoretical concepts in the ML space, with an emphasis on practical and intuitive applications.</em></p> <p><em>Thanks for reading this article! If you have any questions, I will be happy to answer them. Feel free to message me on my </em><a href="http://linkedin.com/in/suresh-raghu"><em>LinkedIn</em></a><em> or my </em><a href="http://sureshraghu0706@gmail.com"><em>email</em></a><em> for other queries.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=31ed580be9d4" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Image aesthetics quantification using OpenAI CLIP</title><link href="https://r-suresh07.github.io/blog/2023/image-aesthetics-quantification-using-openai-clip/" rel="alternate" type="text/html" title="Image aesthetics quantification using OpenAI CLIP"/><published>2023-10-20T11:39:24+00:00</published><updated>2023-10-20T11:39:24+00:00</updated><id>https://r-suresh07.github.io/blog/2023/image-aesthetics-quantification-using-openai-clip</id><content type="html" xml:base="https://r-suresh07.github.io/blog/2023/image-aesthetics-quantification-using-openai-clip/"><![CDATA[<h4>Leveraging the Power of OpenAI CLIP for Quantifying Image Aesthetics in Python</h4> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*S2zlZJuzAaDNVIM1C0dj4w.png"/><figcaption>Source: DALL E-3</figcaption></figure> <p>Recently, I embarked on a project that involved image classification and ranking for a website I was developing. However, I encountered a challenge: the images ranked at the top by typical rule-based systems were not of the highest quality and only had mediocre aesthetics. Believing that there was room for improvement, I turned to the internet for solutions. That’s when I stumbled upon this <a href="https://www.researchgate.net/publication/365734540_CLIP_knows_image_aesthetics">research paper</a>, which discussed how OpenAI’s Zero Shot Classification model, CLIP, could effectively understand various aspects of image aesthetics such as colors, lighting, composition, framing, cropping, and more. Intrigued by the potential of this approach, I decided to implement the paper’s prompting methodology in hopes of achieving superior accuracy with relatively low effort.</p> <p>Full code is present in this google collab:</p> <p><a href="https://colab.research.google.com/drive/1BfBfqHSpPnXDiKJIgv6wjcLGhTkDuCvY?usp=sharing">Google Colaboratory</a></p> <p><strong>What is CLIP?</strong></p> <p><a href="https://openai.com/research/clip">CLIP, an acronym for Contrastive Language-Image Pre-Training</a>, is a neural network developed by OpenAI in January 2021. It’s a multimodal model that merges the concepts of Natural Language Processing and Computer Vision, enabling interaction with images through words by leveraging the knowledge of the English language and the semantic understanding of images.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5UdbvimWYw2rxgLNFTiMeQ.png"/><figcaption>Functionality of CLIP, Source: <a href="https://openai.com/research/clip">CLIP: Connecting text and images (openai.com)</a></figcaption></figure> <p>One of CLIP’s key features is its ability to perform “Zero-shot” classification on images. In essence, it can classify images it has never encountered during its training phase by extrapolating the concepts of the images it learned during training. For instance, if it was trained on a dataset comprising three types of cats: Siamese, Persian, and Maine Coon, and we introduce an unknown breed like an orange tabby cat, CLIP can classify it as a cat even though it has never seen this breed in its dataset. This is a testament to CLIP’s remarkable capabilities.</p> <p>In a way, CLIP brings the flexibility of transformers, akin to ChatGPT 3.5, into the realm of Computer Vision.</p> <p><strong>Problem Statement</strong></p> <p>The goal of this project is to utilize OpenAI’s CLIP model to develop a method for objectively quantifying an image based on its aesthetics. We will be implementing this using Python in a Jupyter Notebook environment.</p> <p><strong>Methodology</strong></p> <pre>!pip install numpy==1.24.4<br />!pip install pandas==2.0.3<br />!pip install Pillow==10.1.0<br />!pip install Requests==2.31.0<br />!pip install streamlit==1.27.2<br />!pip install torch<br />!pip install transformers</pre> <p>First, we will install all the necessary dependencies using pip:</p> <ul><li><strong>Numpy</strong>: This library will be used for efficient numerical operations on arrays.</li><li><strong>Pandas</strong>: We’ll use this for manipulating data in Excel-like tables.</li><li><strong>Pillow</strong>: This will be our go-to library for image processing in Python.</li><li><strong>Requests</strong>: This library will allow us to send HTTP requests using Python and handle the responses. We’ll use it to download images via URLs.</li><li><strong>Streamlit</strong>: This will be used to create easy web apps for our machine learning and data science projects.</li><li><strong>Torch and Transformers</strong>: These libraries will be used to interact with the CLIP model.</li></ul> <p>In the paper, there are 3 types of prompting methodologies:</p> <ul><li>Fixed prompting</li><li>Context-aware prompting</li><li>Ensembling</li></ul> <blockquote>In fixed prompting, we utilise exactly two prompts , one for aesthetic images, one for unaesthetic images. Both prompts are formed using the string template “a [label] picture”, where [label] is either a positive or a negative word from our list of adjectives. Given these two prompts, we ﬁnd the one more similar to the image using CLIP.</blockquote> <p>The authors describe fixed prompting as follows: Essentially, you would have two phrases — “an outstanding picture” as a positive prompt and “an atrocious picture” as a negative prompt. Then, you would compare the image you wish to score with each of these prompts and record the cosine similarity score for each. To compute the total score for an image:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/427/1*3LnxFVQivwEBZGrlFtFBdA.png"/><figcaption>Scoring methodology</figcaption></figure> <p>So if an image is similar to the positive prompt as compared to the negative prompt then the net score of the image will be positive. Higher the score, the more aesthetic the image is.</p> <p><strong>For Context-aware prompting</strong></p> <blockquote>Fixed prompts do not account for the content of the image. Instead of the generic prompt “a beautiful picture”, we hypothesize that it is better to include the content of the image, e.g., “a beautiful picture of a dog”. This speciﬁcation of the text prompt moves the encoded prompt vectors closer toward the image vector, thus reducing noise in similarity measurements.</blockquote> <p>By providing context to an image, we can achieve more accurate scores. In the original paper, the authors used the 1000 class names of the ImageNet dataset for their list of adjectives. However, for our specific domains (in my case, hotels), we would need to create our own dataset of class names relevant to our domains. This can be easily accomplished using ChatGPT. Simply use the following prompt, adjust the domain as needed, and copy and paste the output into an Excel file.</p> <pre>Can you make me a list of {number_of_classes_needed} class names similar to the imagenet class names but which relates to the field of {domain}.</pre> <p>I have generated 1000 class names all relating to hotels as that is my use case and stored them in an excel file called Hotel_Classes.xlsx with the column name being ‘Col_Names’.</p> <p><strong>About Ensembling</strong></p> <blockquote>Our ensembling approach is structurally similar to the context-aware prompts. However, we condense the 2,000 prompts down to two vectors by averaging all prompt vectors of each aesthetic label. The best labels are the same as for the context-aware prompts, but the results show that this method improves the performance while being computationally less expensive, since only two instead of 2,000 comparisons have to be done for each image.</blockquote> <p>Essentially, instead of performing 2000 cosine similarity computations, we average all the positive prompts into a single vector and all the negative prompts into another single vector. This allows us to only perform two cosine similarity computations, which significantly reduces the time it takes to generate a score.</p> <p>Below is a table showcasing the results of all these methods on the <a href="https://github.com/imfing/ava_downloader">Aesthetic Visual Analysis (AVA) dataset</a>. This dataset comprises more than 250,000 images with annotations relating to the aesthetics of each image.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/883/1*rzAaV3REbs3xXAQ_Fl6Wtw.png"/><figcaption>Results showcasing all the methods in the paper.</figcaption></figure> <p>As you can see the Ensembling gives us better accuracy while reducing the number of comparisons needed to be done.</p> <p><strong>Code</strong></p> <pre>from PIL import Image<br />import torch<br />from transformers import AutoProcessor, CLIPModel<br />import torch.nn as nn<br />import requests<br />from io import BytesIO<br />import os<br />import pickle<br />import numpy as np<br />import pandas as pd<br />device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &quot;cpu&quot;)<br />processor = AutoProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br />model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;).to(device)</pre> <p>This code will import the necessary model bin and config files through the Transformers library. Another way of using the model could be through <a href="https://huggingface.co/sentence-transformers">Sentence-Transformers</a> so if you are looking for another way of using CLIP, there you go.</p> <pre>def load_image_PIL(url_or_path):<br />    if url_or_path.startswith(&quot;http://&quot;) or url_or_path.startswith(&quot;https://&quot;):<br />        return Image.open(requests.get(url_or_path, stream=True).raw)<br />    else:<br />        return Image.open(url_or_path)<br />def cosine_similarity(vec1, vec2):<br />    # Compute the dot product of vec1 and vec2<br />    dot_product = np.dot(vec1, vec2)<br />    <br />    # Compute the L2 norm of vec1 and vec2<br />    norm_vec1 = np.linalg.norm(vec1)<br />    norm_vec2 = np.linalg.norm(vec2)<br />    <br />    # Compute the cosine similarity<br />    similarity = dot_product / (norm_vec1 * norm_vec2)<br />    <br />    return similarity</pre> <p>This code provides us with some useful helper functions which will be used later down the line. load_image_PIL() can convert any URL whether it be a local file path or a URL from the internet into a PIL object which is very useful. cosine_similarity() is used to find the cosine similarity between any 2 NumPy vectors.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*OU7Mu7otHm59WiTt.jpg"/><figcaption>Source: https<a href="https://aitechtrend.com/how-cosine-similarity-can-improve-your-machine-learning-models/">://aitechtrend.com/how-cosine-similarity-can-improve-your-machine-learning-models/</a></figcaption></figure> <pre>temp=pd.read_excel(r&quot;Hotel_Classes.xlsx&quot;)<br />classes=temp[&#39;Col_Names&#39;].tolist()<br />classes=[s.lstrip() for s in classes]<br />positive_classes=[]<br />negative_classes=[]<br />for i in range(len(classes)):<br />    positive_classes.append(f&quot;a outstanding picture, of a #{classes[i]}&quot;)<br />    negative_classes.append(f&quot;a horrible picture, of a #{classes[i]}&quot;)</pre> <p>This code imports all the class names we had generated through ChatGPT which we have stored in an excel file into python through pandas. We then convert the data frame into a list of all classes. After that, we create 2 lists called positive_classes and negative_classes. Then we iterate over the classes list and for every class name we insert “a smashing/horrible picture, of a #{class_name}”. The reason why we choose the adjectives “outstanding” for positive and “horrible” for negative is because that is what the researchers found was the most useful.</p> <blockquote>Our prompting results show that CLIP extracts features that can be used for the IAA task. These features correlate with text prompts describing the corresponding aesthetic value of the image. Overall, the labels “outstanding” and “horrible” are well-suited for this task. In an application, it might be possible to get acceptable results if only a pretrained CLIP model is available.</blockquote> <p>Also, the reason why we use #class_name is because CLIP was trained over a lot of internet data by OpenAI and a substantial part of the data was from places like twitter, Tumblr and so on where users post images usually accompanied by hashtags relating to that image, so CLIP learnt this behavior as well.</p> <pre>positive_inputs=processor(text=positive_classes, return_tensors=&quot;pt&quot;, padding=True).to(device)<br />with torch.no_grad():<br />    positive_text_features = model.get_text_features(**positive_inputs)<br />negative_inputs=processor(text=negative_classes, return_tensors=&quot;pt&quot;, padding=True).to(device)<br />with torch.no_grad():<br />    negative_text_features = model.get_text_features(**negative_inputs)</pre> <p>In this code, the positive prompts and the negative prompts are converted into their text features by passing them through the CLIP model.</p> <pre>positive_prompt_vectors = np.array(positive_text_features)<br /># Compute the average vector<br />average_positive_vector = np.mean(positive_prompt_vectors, axis=0)<br /><br />negative_prompt_vectors = np.array(negative_text_features)<br /># Compute the average vector<br />average_negative_vector = np.mean(negative_prompt_vectors, axis=0)</pre> <p>Now we ensemble the 2000 vectors into 2 vectors for easier computation according to the research paper.</p> <pre>with open(&#39;positive_prompt.pkl&#39;, &#39;wb&#39;) as f:<br />    pickle.dump(average_positive_vector, f)<br />with open(&#39;negative_prompt.pkl&#39;, &#39;wb&#39;) as f:<br />    pickle.dump(average_negative_vector, f)</pre> <p>Now lets save these vectors into pickle files so that we could fetch them later when needed. That concludes the preprocessing stage. Simple, isn’t it?</p> <p><strong>Code for runtime</strong></p> <pre>with open(&#39;positive_prompt.pkl&#39;, &#39;rb&#39;) as f:<br />    average_positive_vector = pickle.load(f)<br />with open(&#39;negative_prompt.pkl&#39;, &#39;rb&#39;) as f:<br />    average_negative_vector = pickle.load(f)</pre> <pre>def predict(img_url):<br />    image1 = load_image_PIL(img_url)<br />    with torch.no_grad():<br />        inputs1 = processor(images=image1, return_tensors=&quot;pt&quot;).to(device)<br />        image_features1 = model.get_image_features(**inputs1)<br />    image_vector=image_features1.numpy()<br />    positive_similarity=cosine_similarity(average_positive_vector,np.transpose(image_vector))<br />    negative_similarity=cosine_similarity(average_negative_vector,np.transpose(image_vector))<br />    aesthetic_score=(+1*positive_similarity)+(-1*negative_similarity)<br />    return aesthetic_score*1000     #Multiplied by 1000 just to make it easier to compare scores</pre> <p>We first fetch our vectors from the pickle files. In the predict() function, we first convert the image url passed into a PIL object using the function we defined earlier. We then use CLIP to convert the image object into image features. We convert this image features into NumPy(as they are currently tensors) and apply cosine similarity using the function cosine_similarity() which we defined earlier to get our score. The final score is calculated by multiplying the positive score by +1 and negative score by -1 and sum them together. We then finally return this score to the user. This will be the aesthetic score generated by the program.</p> <pre>import streamlit as st<br />st.header(&#39;Image Aesthetics Scorer&#39;)<br /><br />uploaded_file = st.file_uploader(&quot;Choose an image...&quot;, type=[&#39;png&#39;,&#39;jpg&#39;,&#39;jpeg&#39;])<br />picture_width = st.sidebar.slider(&#39;Picture Width&#39;, min_value=100, max_value=500)<br />if uploaded_file is not None:<br />    image = Image.open(uploaded_file)<br />    st.subheader(&#39;Input&#39;, divider=&#39;rainbow&#39;)<br />    st.image(image, caption=&#39;Uploaded Image&#39;, width=picture_width)<br /><br />        # Call your function with the uploaded image<br />    results = predict(image)<br />    <br />    st.subheader(&#39;Results&#39;, divider=&#39;rainbow&#39;)<br />        # Display the results<br />    st.image(image, caption=results, width=picture_width)</pre> <p>Here is a little bit of Streamlit code which will help us make a demo of this program very easily. If you want to use this, make sure to replace the first line in the predict() function from:</p> <pre>def predict(img_url):<br />    image1 = load_image_PIL(img_url)</pre> <p>to</p> <pre>def predict(img):<br />    image1 = img</pre> <p>When you want to run this program, save the code as a .py file. Let&#39;s say app.py. Open up your command prompt (or wherever you are working with your python environment) and navigate to the place where app.py is stored. Run the preprocessing code once so that positive_prompt.pkl and negative_prompt.pkl are generated and in the same directory as app.py.</p> <p>Then you can finally run the following command:</p> <pre>streamlit run app.py</pre> <p>This will pop out a browser window on the browser of your choice. This is how it should look when its fully functional:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HxmZ6j9L4xx6L4r54Ysi8A.png"/></figure> <p>There is a slider on the left which will affect the size of the output image. When you want to use the app, you just browse and choose a picture and the app will return a score.</p> <p>Now let&#39;s finally do some aesthetics comparisons. Since my use case relates to hotels, I will showcase 3 instances where the program is useful.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gOlNpZKfFB-6dGC6z7e-Cw.jpeg"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*S3VbyUFvXR6jdx6qiRiDcg.jpeg"/><figcaption>2 Images of Hotel Bedrooms</figcaption></figure> <p>As you can see, two pictures of bedrooms are displayed. The one on the left is from a 5-star hotel, while the one on the right is from a budget accommodation. Upon visual inspection, we can easily determine which one appears more aesthetically pleasing. But how would we quantify this? Let’s explore how the program quantifies aesthetics.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/723/1*6nkdEDVIKkXSTle0qlluig.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/723/1*2UO3fn6xygp4flDUoq1NHQ.png"/><figcaption>Bedroom aesthetic score comparison</figcaption></figure> <p>As you can see, the model favors the 5-star bedroom, giving it a rating of approximately 35.5, compared to the budget accommodation, which received a score of only 19.89. This aligns with our own subjective assessment of the aesthetics.</p> <p>Let&#39;s try out another set of scenarios:</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/297/1*iuB40asmY65zPulBrxb4Hg.jpeg"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/930/1*CsqjAgO3GsBz6wYechmnVg.jpeg"/><figcaption>2 images of Hotel Restrooms</figcaption></figure> <p>Here are two images of restrooms from hotels at different price points. The image on the left is from a 5-star hotel, while the one on the right is from a 4-star hotel. From a subjective standpoint, I find the restroom in the 5-star hotel more appealing. The framing and composition appear superior, and the lighting seems to be better compared to the restroom in the 4-star hotel.</p> <p>Now let’s see how the model rates the images.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/742/1*R49KvrGOwrkog41znmysaA.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/746/1*Q9hszavd89ai-V6k-cmhfQ.png"/><figcaption>Restroom aesthetic score comparison</figcaption></figure> <p>It appears that the model aligns with my intuition regarding these images as the 5-star restroom got a score of 27.6 meanwhile the 4-star restroom only got a score of 10.9.</p> <p>For one final comparison, look at the images below.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*eVIlt_bFPhAnmbDmO-DaKg.jpeg"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/318/1*_I4Fx1w9KbpG6bAZUFAMog.jpeg"/><figcaption>2 images of hotel gyms</figcaption></figure> <p>Here are two images of hotel gyms from establishments at different price points. The image on the left is from a 4-star hotel, while the one on the right is from a 5-star hotel.</p> <p>Subjectively speaking, I find the gym in the 5-star hotel more appealing due to its superior framing and the natural lighting that enhances its ambiance. Therefore, I would personally prefer the 5-star gym over the 4-star one.</p> <p>Let’s see if the model shares this preference.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/727/1*ZkKAR46kzv0PD7iedbN4tQ.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/744/1*Rpy590ROYr9AmOntGz_eWQ.png"/><figcaption>Gym aesthetic score comparison</figcaption></figure> <p>As observed, the model significantly favors the 5-star gym over the 4-star gym, with the former receiving a score of 25.46 and the latter a score of -2.28. A negative score indicates that the model does not find the image aesthetically pleasing.</p> <p><strong>Closing Remarks</strong></p> <p>We have successfully demonstrated how to leverage CLIP’s ability to extract image aesthetics and create a scoring system for comparing various images.</p> <p>Although this method did not yield the highest accuracy according to the research paper referenced earlier (0.756 through ensemble prompting compared to the maximum of 0.816 through fine tuning CLIP), the ratio of effort to reward more than compensates for it. Even the authors themselves suggest that using ensemble prompting would be suitable for an application if no other option was available.</p> <h3>Follow For More!</h3> <p><em>I try to implement a lot of theoretical concepts in the ML space, with an emphasis on practical and intuitive applications.</em></p> <p><em>Thanks for reading this article! If you have any questions, I will be happy to answer them. Feel free to message me on my </em><a href="http://linkedin.com/in/suresh-raghu"><em>LinkedIn</em></a><em> or my </em><a href="http://sureshraghu0706@gmail.com/"><em>email</em></a><em> for other queries.</em></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7bbb45e00147" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>